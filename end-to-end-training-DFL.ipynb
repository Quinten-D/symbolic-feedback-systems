{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484dd0e9-f9d0-49d5-aeed-51767aaecd73",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e59a5-bda7-46c6-89f4-696d23e41805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import tqdm # version 4.40.0\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from cpmpy import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3cd03e-cad2-44cd-9f11-326cef1a07af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Make/load the sudoku boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5adff2a-e331-4b7c-98d4-898e657fe4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Make fixed sudoku boards for training and testing -----\n",
    "# train sudoku boards consist of train MNIST images, test sudoku boards of test MNIST images\n",
    "\n",
    "# Sudoku boards\n",
    "# from https://github.com/locuslab/SATNet/blob/master/exps/sudoku.py\n",
    "datadir = \"sudoku\"\n",
    "with open(os.path.join(datadir,'features.pt'), 'rb') as f:\n",
    "    X_in = torch.load(f)\n",
    "with open(os.path.join(datadir,'labels.pt'), 'rb') as f:\n",
    "    Y_in = torch.load(f)\n",
    "\n",
    "# Divide the boards (2000 test, 8000 train)\n",
    "#test_ids = [i for i in range(0, len(X_in), 5)]\n",
    "#train_ids = [i for i in range(0, len(X_in)) if i not in test_ids]\n",
    "train_ids = [i for i in range(0, 7000)]\n",
    "validation_ids = [i for i in range(7000, 8000)]\n",
    "test_ids = [i for i in range(8000, 10000)]\n",
    "\n",
    "# Normalize MNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the MNIST data\n",
    "testset = datasets.MNIST('.', download=True, train=False, transform=transform)\n",
    "digit_indices_test = {k:torch.LongTensor(*np.where(testset.targets == k)) for k in range(0,10)}\n",
    "trainset = datasets.MNIST('.', download=True, train=True, transform=transform)\n",
    "\n",
    "# Make validation set\n",
    "# Initialize counters for each digit class in the validation set\n",
    "count_per_class = [0] * 10\n",
    "# Initialize arrays for the validation set\n",
    "validation_set = []\n",
    "train_set = []\n",
    "# Iterate through the training set and move samples to the validation set\n",
    "for img, label in trainset:\n",
    "    if count_per_class[label] < 1000:\n",
    "        validation_set.append([img, label])\n",
    "        count_per_class[label] += 1\n",
    "    else:\n",
    "        train_set.append([img, label])\n",
    "validation_labels = np.array([x[1] for x in validation_set])\n",
    "digit_indices_validation = {k:torch.LongTensor(*np.where(validation_labels == k)) for k in range(0,10)}\n",
    "train_labels = np.array([x[1] for x in train_set])\n",
    "digit_indices_train = {k:torch.LongTensor(*np.where(train_labels == k)) for k in range(0,10)}\n",
    "\n",
    "\n",
    "# Define constants\n",
    "N_CHANNEL = 1 # MNIST images are in grey scale \n",
    "IMAGE_WIDTH = 28 # MNIST image width\n",
    "IMAGE_HEIGHT = 28 # MNIST image height\n",
    "\n",
    "\n",
    "# fix the seed for board generation\n",
    "torch.manual_seed(9168)\n",
    "np.random.seed(9168)\n",
    "rng = np.random.RandomState(243)\n",
    "\n",
    "\n",
    "def sample_visual_sudoku_test(sudoku_puzzle):\n",
    "    \"\"\"\n",
    "        Turn the given `sudoku_puzzle` into a visual puzzle by replace numeric values\n",
    "        by images from MNIST. \n",
    "    \"\"\"\n",
    "    # Visual Sudoku Tensor of shape puzzle_width x puzzle_height x n_channel x image_width x image_height\n",
    "    sudoku_torch_dimension = sudoku_puzzle.shape + (N_CHANNEL, IMAGE_WIDTH, IMAGE_HEIGHT,)\n",
    "    vizsudoku = torch.zeros(sudoku_torch_dimension, dtype=torch.float32)\n",
    "\n",
    "    # sample dataset indices for each non-zero digit\n",
    "    #for val in np.unique(sudoku_puzzle[sudoku_puzzle > 0]):\n",
    "    for val in np.unique(sudoku_puzzle):\n",
    "        val_idx = np.where(sudoku_puzzle == val)\n",
    "        # randomly sample different MNIST images for a given digit all at once\n",
    "        idx = torch.LongTensor(rng.choice(digit_indices_test[val], len(sudoku_puzzle[val_idx])))\n",
    "        vizsudoku[val_idx] = torch.stack([testset[i][0] for i in idx])\n",
    "        #print(val, idx)\n",
    "    return vizsudoku\n",
    "\n",
    "\n",
    "def sample_visual_sudoku_test_correlated(sudoku_puzzle):\n",
    "    \"\"\"\n",
    "        Turn the given `sudoku_puzzle` into a visual puzzle by replace numeric values\n",
    "        by images from MNIST. \n",
    "    \"\"\"\n",
    "    # Visual Sudoku Tensor of shape puzzle_width x puzzle_height x n_channel x image_width x image_height\n",
    "    sudoku_torch_dimension = sudoku_puzzle.shape + (N_CHANNEL, IMAGE_WIDTH, IMAGE_HEIGHT,)\n",
    "    vizsudoku = torch.zeros(sudoku_torch_dimension, dtype=torch.float32)\n",
    "\n",
    "    # sample dataset indices for each non-zero digit\n",
    "    #for val in np.unique(sudoku_puzzle[sudoku_puzzle > 0]):\n",
    "    for val in np.unique(sudoku_puzzle):\n",
    "        val_idx = np.where(sudoku_puzzle == val)\n",
    "        # randomly sample different MNIST images for a given digit all at once\n",
    "        idx = torch.LongTensor(rng.choice(digit_indices_test[val], len(sudoku_puzzle[val_idx])))\n",
    "        # Add noise\n",
    "        images_with_noise = [add_noise(testset[idx[0]][0], noise_level=0.75) for i in idx]\n",
    "        vizsudoku[val_idx] = torch.stack([torch.tensor(img) for img in images_with_noise])\n",
    "    return vizsudoku\n",
    "\n",
    "\n",
    "def sample_visual_sudoku_train(sudoku_puzzle):\n",
    "    \"\"\"\n",
    "        Turn the given `sudoku_puzzle` into a visual puzzle by replace numeric values\n",
    "        by images from MNIST. \n",
    "    \"\"\"\n",
    "    # Visual Sudoku Tensor of shape puzzle_width x puzzle_height x n_channel x image_width x image_height\n",
    "    sudoku_torch_dimension = sudoku_puzzle.shape + (N_CHANNEL, IMAGE_WIDTH, IMAGE_HEIGHT,)\n",
    "    vizsudoku = torch.zeros(sudoku_torch_dimension, dtype=torch.float32)\n",
    "\n",
    "    # sample dataset indices for each non-zero digit\n",
    "    #for val in np.unique(sudoku_puzzle[sudoku_puzzle > 0]):\n",
    "    for val in np.unique(sudoku_puzzle):\n",
    "        val_idx = np.where(sudoku_puzzle == val)\n",
    "        # randomly sample different MNIST images for a given digit all at once\n",
    "        idx = torch.LongTensor(rng.choice(digit_indices_train[val], len(sudoku_puzzle[val_idx])))\n",
    "        vizsudoku[val_idx] = torch.stack([train_set[i][0] for i in idx])\n",
    "        #print(val, idx)\n",
    "    return vizsudoku\n",
    "\n",
    "\n",
    "def sample_visual_sudoku_validation(sudoku_puzzle):\n",
    "    \"\"\"\n",
    "        Turn the given `sudoku_puzzle` into a visual puzzle by replace numeric values\n",
    "        by images from MNIST. \n",
    "    \"\"\"\n",
    "    # Visual Sudoku Tensor of shape puzzle_width x puzzle_height x n_channel x image_width x image_height\n",
    "    sudoku_torch_dimension = sudoku_puzzle.shape + (N_CHANNEL, IMAGE_WIDTH, IMAGE_HEIGHT,)\n",
    "    vizsudoku = torch.zeros(sudoku_torch_dimension, dtype=torch.float32)\n",
    "\n",
    "    # sample dataset indices for each non-zero digit\n",
    "    #for val in np.unique(sudoku_puzzle[sudoku_puzzle > 0]):\n",
    "    for val in np.unique(sudoku_puzzle):\n",
    "        val_idx = np.where(sudoku_puzzle == val)\n",
    "        # randomly sample different MNIST images for a given digit all at once\n",
    "        idx = torch.LongTensor(rng.choice(digit_indices_validation[val], len(sudoku_puzzle[val_idx])))\n",
    "        vizsudoku[val_idx] = torch.stack([validation_set[i][0] for i in idx])\n",
    "        #print(val, idx)\n",
    "    return vizsudoku\n",
    "\n",
    "\n",
    "def sample_visual_sudoku_validation_correlated(sudoku_puzzle):\n",
    "    \"\"\"\n",
    "        Turn the given `sudoku_puzzle` into a visual puzzle by replace numeric values\n",
    "        by images from MNIST. \n",
    "    \"\"\"\n",
    "    # Visual Sudoku Tensor of shape puzzle_width x puzzle_height x n_channel x image_width x image_height\n",
    "    sudoku_torch_dimension = sudoku_puzzle.shape + (N_CHANNEL, IMAGE_WIDTH, IMAGE_HEIGHT,)\n",
    "    vizsudoku = torch.zeros(sudoku_torch_dimension, dtype=torch.float32)\n",
    "\n",
    "    # sample dataset indices for each non-zero digit\n",
    "    #for val in np.unique(sudoku_puzzle[sudoku_puzzle > 0]):\n",
    "    for val in np.unique(sudoku_puzzle):\n",
    "        val_idx = np.where(sudoku_puzzle == val)\n",
    "        # randomly sample different MNIST images for a given digit all at once\n",
    "        idx = torch.LongTensor(rng.choice(digit_indices_validation[val], len(sudoku_puzzle[val_idx])))\n",
    "        # Add noise\n",
    "        images_with_noise = [add_noise(validation_set[idx[0]][0], noise_level=0.75) for i in idx]\n",
    "        vizsudoku[val_idx] = torch.stack([torch.tensor(img) for img in images_with_noise])\n",
    "    return vizsudoku\n",
    "\n",
    "\n",
    "# Define a function to add random noise to an image\n",
    "def add_noise(image, noise_level=0.1):\n",
    "    noise = np.random.normal(scale=noise_level, size=image.shape)\n",
    "    noisy_image = image + noise\n",
    "    return torch.tensor(noisy_image.clip(-1, 1), dtype=torch.float32).clone().detach()  # Ensure values are within [-1, 1] range\n",
    "\n",
    "\n",
    "def transform_labels(sudoku):\n",
    "    new_labels = torch.zeros(9, 9)\n",
    "    tensor_digits = torch.arange(1, 10)\n",
    "    for i in range(0, 9):\n",
    "        for j in range(0, 9):\n",
    "            digit_vector = sudoku[i, j]\n",
    "            new_labels[i, j] = torch.sum(tensor_digits * digit_vector)\n",
    "    return new_labels.numpy().astype(int)\n",
    "\n",
    "\n",
    "# ----- Do the test sudokus -----\n",
    "visual_test_sudokus = []\n",
    "numerical_test_sudokus = []\n",
    "completed_numerical_test_sudokus = []\n",
    "\n",
    "for i in test_ids:\n",
    "    completed_numerical_sudoku = torch.from_numpy(transform_labels(Y_in[i]))\n",
    "    numerical_sudoku = torch.from_numpy(transform_labels(X_in[i]))\n",
    "    visual_sudoku = sample_visual_sudoku_test(transform_labels(X_in[i]))\n",
    "    visual_test_sudokus.append(visual_sudoku)\n",
    "    numerical_test_sudokus.append(numerical_sudoku)\n",
    "    completed_numerical_test_sudokus.append(completed_numerical_sudoku)\n",
    " \n",
    "# torch.save(visual_test_sudokus, 'visual_test_sudokus.pth')\n",
    "# torch.save(numerical_test_sudokus, 'numerical_test_sudokus.pth')\n",
    "# torch.save(completed_numerical_test_sudokus, 'completed_numerical_test_sudokus.pth')\n",
    "\n",
    "\n",
    "# ----- Do the correlated test sudokus -----\n",
    "visual_correlated_test_sudokus = []\n",
    "\n",
    "for i in test_ids:\n",
    "    completed_numerical_sudoku = torch.from_numpy(transform_labels(Y_in[i]))\n",
    "    numerical_sudoku = torch.from_numpy(transform_labels(X_in[i]))\n",
    "    visual_sudoku = sample_visual_sudoku_test_correlated(transform_labels(X_in[i]))\n",
    "    visual_correlated_test_sudokus.append(visual_sudoku)\n",
    " \n",
    " \n",
    "# ----- Do the train sudokus -----\n",
    "visual_train_sudokus = []\n",
    "numerical_train_sudokus = []\n",
    "completed_numerical_train_sudokus = []\n",
    "\n",
    "#for i in train_ids:\n",
    "double_train_ids = train_ids + train_ids + train_ids + train_ids + train_ids + train_ids\n",
    "#double_train_ids = 15 * train_ids\n",
    "for i in double_train_ids:\n",
    "    completed_numerical_sudoku = torch.from_numpy(transform_labels(Y_in[i]))\n",
    "    numerical_sudoku = torch.from_numpy(transform_labels(X_in[i]))\n",
    "    visual_sudoku = sample_visual_sudoku_train(transform_labels(X_in[i]))\n",
    "    visual_train_sudokus.append(visual_sudoku)\n",
    "    numerical_train_sudokus.append(numerical_sudoku)\n",
    "    completed_numerical_train_sudokus.append(completed_numerical_sudoku)\n",
    "\n",
    "# torch.save(visual_train_sudokus, 'visual_train_sudokus.pth')\n",
    "# torch.save(numerical_train_sudokus, 'numerical_train_sudokus.pth')\n",
    "# torch.save(completed_numerical_train_sudokus, 'completed_numerical_train_sudokus.pth')\n",
    "\n",
    "\n",
    "# ----- Do the validation sudokus -----\n",
    "visual_validation_sudokus = []\n",
    "numerical_validation_sudokus = []\n",
    "completed_numerical_validation_sudokus = []\n",
    "\n",
    "for i in validation_ids:\n",
    "    completed_numerical_sudoku = torch.from_numpy(transform_labels(Y_in[i]))\n",
    "    numerical_sudoku = torch.from_numpy(transform_labels(X_in[i]))\n",
    "    visual_sudoku = sample_visual_sudoku_validation(transform_labels(X_in[i]))\n",
    "    visual_validation_sudokus.append(visual_sudoku)\n",
    "    numerical_validation_sudokus.append(numerical_sudoku)\n",
    "    completed_numerical_validation_sudokus.append(completed_numerical_sudoku)\n",
    "\n",
    "# torch.save(visual_validation_sudokus, 'visual_validation_sudokus.pth')\n",
    "# torch.save(numerical_validation_sudokus, 'numerical_validation_sudokus.pth')\n",
    "# torch.save(completed_numerical_validation_sudokus, 'completed_numerical_validation_sudokus.pth')\n",
    "\n",
    "\n",
    "# ----- Do the correlated validation sudokus -----\n",
    "visual_correlated_validation_sudokus = []\n",
    "\n",
    "for i in validation_ids:\n",
    "    completed_numerical_sudoku = torch.from_numpy(transform_labels(Y_in[i]))\n",
    "    numerical_sudoku = torch.from_numpy(transform_labels(X_in[i]))\n",
    "    visual_sudoku = sample_visual_sudoku_validation_correlated(transform_labels(X_in[i]))\n",
    "    visual_correlated_validation_sudokus.append(visual_sudoku)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ad1f7-852d-4b34-a9dc-993737f76452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define a helper function to visualize the visual puzzle\n",
    "\n",
    "N_CHANNEL = 1 # MNIST images are in grey scale \n",
    "IMAGE_WIDTH = 28 # MNIST image width\n",
    "IMAGE_HEIGHT = 28 # MNIST image height\n",
    "\n",
    "def show_grid_img(visual_sudoku, in_green=None, in_red=None, title=None):\n",
    "    images =visual_sudoku.reshape(-1, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    images = (255 * images).int()\n",
    "    dim = visual_sudoku.shape[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(dim,dim), axes_pad=0.03)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # cells to plot in green | red\n",
    "    N = len(images)\n",
    "    if in_green is None:\n",
    "        in_green = np.zeros(N, dtype=bool)\n",
    "    if in_red is None:\n",
    "        in_red = np.zeros(N, dtype=bool)\n",
    "    in_green = in_green.flatten()\n",
    "    in_red = in_red.flatten()\n",
    "    \n",
    "    for ax, index in zip(grid, range(len(images))):\n",
    "        # dont display ticks\n",
    "        for axis in ax.axis.values():\n",
    "            axis.toggle(ticks=False, ticklabels=False)\n",
    "        # color appropriately\n",
    "        color = 'gray_r'\n",
    "        if in_red[index]:\n",
    "            color = 'autumn'\n",
    "        if in_green[index]:\n",
    "            color = 'summer'\n",
    "        # and show\n",
    "        ax.imshow(images[index].numpy().squeeze(), cmap=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c452581-8204-48d2-be4a-2a505f58bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some puzzles\n",
    "show_grid_img(visual_train_sudokus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8818cb-de5f-4c0b-8a28-0083eba5a620",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-train CNN & SLN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09993bfe-eed0-4274-9fbb-2f8e0fd199f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Digit level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea0699-d237-48ce-aac0-28a57ca46c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Neural Network for digit classification\n",
    "\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TinyLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyLeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 5, 1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(1, 2, 5, 1)\n",
    "        self.fc1 = nn.Linear(5*5*2, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5*5*2) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_classifier(clf_classname, path):\n",
    "    \"\"\"\n",
    "        Initialize a new CNN classifier by\n",
    "        loading pre-trained weights stored in `path` file\n",
    "    \"\"\"\n",
    "    net = clf_classname()\n",
    "    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "    net.load_state_dict(state_dict)\n",
    "    return net\n",
    "\n",
    "def load_classifier2(clf_classname, path):\n",
    "    \"\"\"\n",
    "        Initialize a new CNN classifier by\n",
    "        loading pre-trained weights stored in `path` file\n",
    "    \"\"\"\n",
    "    net = clf_classname()\n",
    "    net_state_dict = net.state_dict()\n",
    "    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "    state_dict = {k: v for k, v in state_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(state_dict)\n",
    "    net.load_state_dict(net_state_dict)\n",
    "\n",
    "    # Freeze loaded weights\n",
    "    for name, param in net.named_parameters():\n",
    "        if name in state_dict:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba_sudoku(neuralnet, vizsudoku):\n",
    "    \"\"\"\n",
    "        Assign a probabilistic vector to each image of the visual puzzle\n",
    "    \"\"\"\n",
    "    for param in neuralnet.parameters():\n",
    "        device = param.device\n",
    "        break  # Only need to check the device of one parameter\n",
    "        \n",
    "    grid_shape = vizsudoku.shape[:2]\n",
    "    # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "    #pred = neuralnet(vizsudoku.flatten(0,1))\n",
    "    # Don't do this since board level\n",
    "    pred = neuralnet(vizsudoku.unsqueeze(0).to(device))\n",
    "    pred = pred.view(9,9,10)\n",
    "    pred = F.softmax(pred, dim=2)\n",
    "    pred = pred.cpu()  # Move the tensor to the CPU\n",
    "    # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "    return pred.reshape(*grid_shape,10).detach() # reshape as 9x9x10 tensor for easier visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a19e28-8f2a-4ea6-94ba-1e2429a15c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "\n",
    "\n",
    "# Set the seed for NumPy's random number generator\n",
    "np.random.seed(1232)\n",
    "\n",
    "\n",
    "class CustomCNNDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.data[idx][1]\n",
    "        img = self.data[idx][0]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf716a-bb7e-4784-84af-955cdad404dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train digit level CNN\n",
    "\n",
    "\n",
    "# Seed CNN\n",
    "torch.manual_seed(4287)\n",
    "\n",
    "\n",
    "# Make dataloaders\n",
    "cnn_trainset = CustomCNNDataset(train_set)\n",
    "cnn_testset = CustomCNNDataset(validation_set)\n",
    "cnn_train_loader = DataLoader(cnn_trainset, batch_size=64, shuffle=True)\n",
    "cnn_test_loader = DataLoader(cnn_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the neural network and define the loss function and optimizer\n",
    "cnn = TinyLeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.0005)\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    cnn.train()\n",
    "    for i, data in enumerate(cnn_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(cnn_train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Print test loss\n",
    "            cnn.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_test_loss = 0.0\n",
    "            batches = 0\n",
    "            with torch.no_grad():\n",
    "                for data in cnn_test_loader:\n",
    "                    inputs, labels = data\n",
    "                    outputs = cnn(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    running_test_loss += loss.item()\n",
    "                    batches += 1\n",
    "        \n",
    "            accuracy = 100 * correct / total\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] - Test Accuracy: {accuracy:.2f}% - Test Loss: {running_test_loss/batches:.4f}')\n",
    "\n",
    "    \n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82a5a-4d31-4cfc-91aa-41c7c97e87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy\n",
    "\n",
    "torch.manual_seed(9168)\n",
    "np.random.seed(9168)\n",
    "\n",
    "cnn_testset = CustomCNNDataset(testset)\n",
    "cnn_test_loader = DataLoader(cnn_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "cnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "running_test_loss = 0.0\n",
    "batches = 0\n",
    "with torch.no_grad():\n",
    "    for data in cnn_test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_test_loss += loss.item()\n",
    "        batches += 1\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}% - Test Loss: {running_test_loss/batches:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14500a0-60bb-47cd-83a8-429ea0e1db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy of CNN on digits of the visual sudoku boards\n",
    "\n",
    "total = 0\n",
    "cnn_correct = 0\n",
    "for i in range(0, 2000):\n",
    "    numerical_sudoku = numerical_test_sudokus[i]\n",
    "    visual_sudoku = visual_test_sudokus[i]\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        device = param.device\n",
    "        break  # Only need to check the device of one parameter\n",
    "    grid_shape = visual_sudoku.shape[:2]\n",
    "    # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "    pred = cnn(visual_sudoku.flatten(0,1))\n",
    "    #pred = cnn(visual_sudoku.unsqueeze(0).to(device))\n",
    "    pred = pred.view(9,9,10)\n",
    "    pred = F.softmax(pred, dim=2)\n",
    "    pred = pred.cpu()  # Move the tensor to the CPU\n",
    "    # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "    ml_predictions =  pred.reshape(*grid_shape,10).detach() # reshape as 9x9x10 tensor for easier visualisation\n",
    "    \n",
    "    #ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "\n",
    "    _, cnn_prediction = torch.max(torch.tensor(ml_predictions).data, -1)\n",
    "    cnn_prediction = cnn_prediction.numpy()\n",
    "    non_zero = (numerical_sudoku != 0).sum().item()\n",
    "    cnn_correct += ((cnn_prediction == numerical_sudoku.numpy())).sum().item()\n",
    "    \n",
    "    total += 81\n",
    "print(i, cnn_correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640175d-22a2-484f-b26b-702b713ca4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual saving\n",
    "\n",
    "torch.save(cnn.state_dict(), 'TinyLeNet_0.9.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbe3e0-1434-4dda-b42c-b47f81e42f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual loading\n",
    "\n",
    "cnn = LeNet()\n",
    "cnn.load_state_dict(torch.load('LeNet_0.944.pth'))\n",
    "cnn.eval()\n",
    "\n",
    "# Count the number of weights\n",
    "num_weights = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
    "print(\"Number of weights in the model:\", num_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7e84e-9e9f-47c1-ad30-624df7061bf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Board level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb46f3a-bb92-45fa-8eb6-b9247de8b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network for board classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class PieceByPieceBoardCNN10(nn.Module):\n",
    "    def __init__(self, use_softmax=False):\n",
    "        super(PieceByPieceBoardCNN10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, 1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, 1)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "        self.fc4 = nn.Linear(81*10, 900)\n",
    "        self.fc5 = nn.Linear(900, 900)\n",
    "        #self.fc6 = nn.Linear(900, 900)\n",
    "        self.fc7 = nn.Linear(900, 810)\n",
    "\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "    def piece_by_piece(self, batch):\n",
    "        # Expect shape (batchx9x9x1x28x28)\n",
    "        # Turn into shape (batchx81x120) and then (batchx9720)\n",
    "        device = next(self.parameters()).device\n",
    "        return_tensor = torch.Tensor([]).to(device)\n",
    "        batch = batch.to(device)\n",
    "        for x in batch:\n",
    "            # First turn 9x9x1x28x28 into 81x1x28x28 \n",
    "            x = x.flatten(0,1)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 5*5*16) \n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            return_tensor = torch.cat((return_tensor, x), 0)\n",
    "        return return_tensor.view(-1, 81*10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.piece_by_piece(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        #x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        if self.use_softmax:\n",
    "            x = x.view(-1, 9, 9, 10)\n",
    "            x = torch.nn.functional.softmax(x, dim=len(x.shape)-1)\n",
    "            x = x.view(-1, 810)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c0ab3-9530-4366-a562-8b11ee73e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataloaders\n",
    "\n",
    "def reshape_image_tensor(input_tensor):\n",
    "    output_tensor = torch.zeros(1, 252, 252)\n",
    "    # Loop through the original tensor and fill the new tensor\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            start_row = i * 28\n",
    "            start_col = j * 28\n",
    "            end_row = start_row + 28\n",
    "            end_col = start_col + 28\n",
    "            output_tensor[:, start_row:end_row, start_col:end_col] = input_tensor[i, j, 0, :, :]\n",
    "    return output_tensor\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomSudokuDataset(Dataset):\n",
    "    def __init__(self, image_dataset, label_dataset):\n",
    "        self.image_data = image_dataset\n",
    "        self.label_data = label_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = reshape_image_tensor(self.image_data[idx])\n",
    "        label = (self.label_data[idx]).view(-1)\n",
    "        return img, label\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomSudokuDatasetPieceByPiece(Dataset):\n",
    "    def __init__(self, image_dataset, label_dataset):\n",
    "        self.image_data = image_dataset\n",
    "        self.label_data = label_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.image_data[idx]\n",
    "        label = (self.label_data[idx]).view(-1)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef631d-df2c-4383-b6ec-263d1dbbf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "def entropy(logits):\n",
    "    # First softmax\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def MyLoss(inputs, targets, device):\n",
    "    # Inputs of shape batch x 810, targets of shape batch x 9 x 9\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_sudoku_loss = 0\n",
    "    for i in range(0, inputs.shape[0]):\n",
    "       # Normal loss\n",
    "       input = inputs[i]\n",
    "       target = targets[i]\n",
    "       loss = criterion(input.view(81, 10), target.view(81))\n",
    "       total_loss += loss\n",
    "        \n",
    "       # Sudoku loss\n",
    "       input = input.view(9, 9, 10)\n",
    "       # For all rows\n",
    "       entropy_sum = 0\n",
    "       sudoku_loss_rows = 0\n",
    "       for row in input:\n",
    "           average_row = torch.mean(row, dim=0)\n",
    "           entropy_sum += entropy(average_row)\n",
    "       sudoku_loss_rows += -entropy_sum\n",
    "       # For all columns\n",
    "       entropy_sum = 0\n",
    "       sudoku_loss_cols = 0\n",
    "       for i in range(input.shape[1]):  # Iterate over columns\n",
    "           column = input[:, i, :]\n",
    "           average_column = torch.mean(column, dim=1)\n",
    "           entropy_sum += entropy(average_column)\n",
    "       sudoku_loss_cols += -entropy_sum\n",
    "       # For all blocks\n",
    "       entropy_sum = 0\n",
    "       sudoku_loss_blocks = 0\n",
    "       for a in range(0, 3):\n",
    "           for b in range(0, 3):\n",
    "               average_block = torch.zeros_like(input[0][0]).to(device)\n",
    "               for c in range(a, a+3):\n",
    "                   for d in range(b, b+3):\n",
    "                       average_block += input[c][d]\n",
    "               average_block = (1/9) * average_block\n",
    "               entropy_sum += entropy(average_block)\n",
    "       sudoku_loss_blocks += -entropy_sum\n",
    "       # For all cells\n",
    "       sudoku_loss_cells = 0\n",
    "       for a in range(0, 9):\n",
    "           for b in range(0, 9):\n",
    "               sudoku_loss_cells += entropy(input[a][b])\n",
    "       # Add them together\n",
    "       sudoku_loss = 0.1*(sudoku_loss_rows + sudoku_loss_cols + sudoku_loss_cols) + 0.1*sudoku_loss_cells\n",
    "       #sudoku_loss = sudoku_loss_rows\n",
    "       total_loss += sudoku_loss\n",
    "       total_sudoku_loss += sudoku_loss\n",
    "        \n",
    "    return total_loss/inputs.shape[0], total_sudoku_loss/inputs.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b3103-5c16-42ac-9dbe-e145bef99979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "def computeSudokuLoss(input):\n",
    "    # Sudoku loss for rows\n",
    "    row_average = torch.mean(input, dim=0)\n",
    "    sudoku_loss_rows = -torch.sum(entropy(row_average))\n",
    "\n",
    "    # Sudoku loss for columns\n",
    "    column_average = torch.mean(input, dim=1)\n",
    "    sudoku_loss_cols = -torch.sum(entropy(column_average))\n",
    "\n",
    "    # Sudoku loss for blocks\n",
    "    sudoku_loss_blocks = 0\n",
    "    for a in range(0, 3):\n",
    "        for b in range(0, 3):\n",
    "            block = input[a:a+3, b:b+3, :]\n",
    "            block_average = torch.mean(block, dim=(0, 1))\n",
    "            sudoku_loss_blocks += -torch.sum(entropy(block_average))\n",
    "\n",
    "    # Sudoku loss for cells\n",
    "    sudoku_loss_cells = torch.sum(entropy(input))\n",
    "\n",
    "    # Combine losses\n",
    "    sudoku_loss = 0.01 * (sudoku_loss_rows + sudoku_loss_cols + sudoku_loss_blocks) + 0.01 * sudoku_loss_cells\n",
    "\n",
    "    return sudoku_loss\n",
    "\n",
    "\n",
    "def entropy(x):\n",
    "    # Assuming x is probabilities along the last dimension\n",
    "    x_softmax = torch.softmax(x, dim=-1)\n",
    "    return -torch.sum(x_softmax * torch.log(x_softmax + 1e-10), dim=-1)\n",
    "\n",
    "\n",
    "def MyLossFast(inputs, targets, use_sudoku_loss=True):\n",
    "    # Inputs of shape batch x 810, targets of shape batch x 9 x 9\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_sudoku_loss = 0\n",
    "    for i in range(0, inputs.shape[0]):\n",
    "        # Normal loss\n",
    "        input = inputs[i]\n",
    "        target = targets[i]\n",
    "        loss = criterion(input.view(81, 10), target.view(81))\n",
    "        total_loss += loss\n",
    "\n",
    "        if use_sudoku_loss:\n",
    "            # Sudoku loss\n",
    "            sudoku_loss = computeSudokuLoss(input.view(9, 9, 10))\n",
    "        else:\n",
    "            sudoku_loss = torch.tensor(0.0)\n",
    "       \n",
    "        total_loss += sudoku_loss\n",
    "        total_sudoku_loss += sudoku_loss\n",
    "        \n",
    "    return total_loss/inputs.shape[0], total_sudoku_loss/inputs.shape[0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "#input = torch.randn(9, 9, 10)  # Assuming input shape is (9, 9, 10)\n",
    "#loss = sudoku_loss(input)\n",
    "#print(loss.item())  # Convert loss tensor to scalar\n",
    "\n",
    "# input = torch.randn(5, 9, 9, 10)\n",
    "# loss, sudoku_loss = MyLossFast(input, torch.ones(5, 9, 9).to(int))\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3581e-5371-4bf4-b1fd-cd7df629eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you can accelerate training\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    device = mps_device\n",
    "    print(\"Device: \", device)\n",
    "    #x = torch.ones(1, device=mps_device)\n",
    "    #print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee500d7-17a7-4785-a8c0-446df91eec9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the board level CNN\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# Make dataloaders\n",
    "batch_size = 16\n",
    "\n",
    "# cnn_trainset = CustomSudokuDataset(visual_train_sudokus, numerical_train_sudokus)\n",
    "# cnn_testset = CustomSudokuDataset(visual_test_sudokus, numerical_test_sudokus)\n",
    "# cnn_validationset = CustomSudokuDataset(visual_validation_sudokus, numerical_validation_sudokus)\n",
    "\n",
    "cnn_trainset = CustomSudokuDatasetPieceByPiece(visual_train_sudokus, numerical_train_sudokus)\n",
    "cnn_testset = CustomSudokuDatasetPieceByPiece(visual_test_sudokus, numerical_test_sudokus)\n",
    "cnn_validationset = CustomSudokuDatasetPieceByPiece(visual_validation_sudokus, numerical_validation_sudokus)\n",
    "\n",
    "cnn_train_loader = DataLoader(cnn_trainset, batch_size=batch_size, shuffle=True)\n",
    "cnn_test_loader = DataLoader(cnn_testset, batch_size=1, shuffle=False)\n",
    "cnn_validation_loader = DataLoader(cnn_validationset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the neural network and define the loss function and optimizer\n",
    "#trained_cnn = SudokuCNN2().to(device)\n",
    "trained_cnn = PieceByPieceBoardCNN10().to(device)\n",
    "trained_cnn.load_state_dict(torch.load('PieceByPieceBoardCNN10_acc_0.966.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(trained_cnn.parameters(), lr=0.000001)\n",
    "num_epochs = 40\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_sudoku_loss = 0.0\n",
    "    trained_cnn.train()\n",
    "    for i, data in enumerate(cnn_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = trained_cnn(inputs)\n",
    "\n",
    "        if epoch >= 40:\n",
    "            loss, sudoku_loss = MyLossFast(outputs, labels, use_sudoku_loss=True)\n",
    "        else:\n",
    "            loss, sudoku_loss = MyLossFast(outputs, labels, use_sudoku_loss=False)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_sudoku_loss += sudoku_loss.item()\n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(cnn_train_loader)}], Loss: {running_loss/50:.4f}, Sudoku Loss: {running_sudoku_loss/50:.4f}')\n",
    "            running_loss = 0.0\n",
    "            running_sudoku_loss = 0.0\n",
    "\n",
    "        if i % 200 == 199:\n",
    "          # Print test loss\n",
    "          correct = 0\n",
    "          total = 0\n",
    "          running_test_loss = 0.0\n",
    "          running_sudoku_test_loss = 0.0\n",
    "          batches = 0\n",
    "          with torch.no_grad():\n",
    "              for data in cnn_validation_loader:\n",
    "                  inputs, labels = data\n",
    "                  inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "                  outputs = trained_cnn(inputs)\n",
    "                  loss, sudoku_loss = MyLossFast(outputs, labels, use_sudoku_loss=False)\n",
    "                  outputs = outputs.view(81, 10)\n",
    "                  labels = labels.view(81)\n",
    "                  # loss = criterion(outputs, labels)\n",
    "                  _, predicted = torch.max(outputs.data, 1)\n",
    "                  total += labels.size(0)\n",
    "                  correct += (predicted == labels).sum().item()\n",
    "                  running_test_loss += loss.item()\n",
    "                  running_sudoku_test_loss += sudoku_loss.item()\n",
    "                  batches += 1\n",
    "                  if batches > 100:\n",
    "                      break\n",
    "          \n",
    "          accuracy = 100 * correct / total\n",
    "          print(f'Epoch [{epoch+1}/{num_epochs}] - Test Accuracy: {accuracy:.2f}% - Test Loss: {running_test_loss/batches:.4f} - Sudoku test Loss: {running_sudoku_test_loss/batches:.4f}')\n",
    "            \n",
    "      # if i==450:\n",
    "      #      break\n",
    "    \n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0f734-1b9b-44ca-a00d-8ad4cb06f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "\n",
    "trained_cnn = PieceByPieceBoardCNN10().to(device)\n",
    "trained_cnn.load_state_dict(torch.load('PieceByPieceBoardCNN10_acc_0.966.pth'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "running_test_loss = 0.0\n",
    "running_sudoku_test_loss = 0.0\n",
    "batches = 0\n",
    "with torch.no_grad():\n",
    "  for data in cnn_test_loader:\n",
    "      inputs, labels = data\n",
    "      inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "      outputs = trained_cnn(inputs)\n",
    "      loss, sudoku_loss = MyLossFast(outputs, labels, use_sudoku_loss=False)\n",
    "      outputs = outputs.view(81, 10)\n",
    "      labels = labels.view(81)\n",
    "      # loss = criterion(outputs, labels)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "      running_test_loss += loss.item()\n",
    "      running_sudoku_test_loss += sudoku_loss.item()\n",
    "      batches += 1\n",
    "      # if batches > 100:\n",
    "      #     break\n",
    "      \n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}% - Test Loss: {running_test_loss/batches:.4f} - Sudoku test Loss: {running_sudoku_test_loss/batches:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fb3a9-07e6-4107-93f4-3a83f7e6499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual saving\n",
    "\n",
    "torch.save(trained_cnn.state_dict(), 'PieceByPieceBoardCNN9_acc_0.9.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60817d-ed05-41e8-910b-0605b4937d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual loading\n",
    "\n",
    "cnn = PieceByPieceBoardCNN9()\n",
    "cnn.load_state_dict(torch.load('PieceByPieceBoardCNN9_acc_0.968.pth'))\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5091e9-d086-4d44-8500-4cccfdf5b903",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sudoku solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84aef19-f64c-4ec0-9360-15099f11ce1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037487c-85e4-4c89-bbe2-e07e67fd099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own model\n",
    "\n",
    "import cpmpy as cp\n",
    "from cpmpy.solvers import CPM_ortools\n",
    "\n",
    "\n",
    "def get_sudoku_model(n=9):\n",
    "    b = np.sqrt(n).astype(int)\n",
    "    cells = cp.IntVar(1, n, shape=(n,n))\n",
    "\n",
    "    # plain sudoku model\n",
    "    m = cp.Model(\n",
    "        [cp.alldifferent(row) for row in cells],\n",
    "        [cp.alldifferent(col) for col in cells.T],\n",
    "        [cp.alldifferent(cells[i:i + b, j:j + b])\n",
    "            for i in range(0, n, b) for j in range(0, n, b)],\n",
    "    )\n",
    "    return {\n",
    "        'model':m,\n",
    "        'variables':cells\n",
    "    }\n",
    "\n",
    "\n",
    "def solve_sudoku(model, dvars, instance):\n",
    "    # use another object for solving\n",
    "    newmodel = cp.Model(model.constraints) \n",
    "    # set given clues\n",
    "    newmodel += cp.all(instance[instance>0] == dvars[instance>0])\n",
    "    if newmodel.solve():\n",
    "        results = {\n",
    "            'runtime':np.asarray(newmodel.cpm_status.runtime),\n",
    "            'solution':dvars.value(),\n",
    "        }\n",
    "    else:\n",
    "        results = {\n",
    "            'solution':np.full_like(dvars.value(), np.nan)\n",
    "        }\n",
    "    results['status'] = np.asarray(newmodel.cpm_status.exitstatus.value)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_visual_sudoku_full_image_model(ml_predictions, precision=1e-5):\n",
    "    # base model and decision variables \n",
    "    visual_sudoku_problem = get_sudoku_model()\n",
    "    model = visual_sudoku_problem['model']\n",
    "    decision_variables = visual_sudoku_problem['variables']\n",
    "    # introduce a layer of 'perception' variables, as an interface\n",
    "    # between the solver and the ml network\n",
    "    # their domain is [0, ..., 9], with 0 acting as the 'empty' symbol\n",
    "    perception_variables = cp.intvar(0, 9, shape=decision_variables.shape, name='perception')\n",
    "\n",
    "    # convert predictions to logspace\n",
    "    logprobs = np.log(np.maximum( ml_predictions, precision ))\n",
    "    # cp solver requires integer values\n",
    "    logprobs = np.array(logprobs / precision).astype(int)\n",
    "    # switch to cpm_array for more features\n",
    "    logprobs = cp.cpm_array(logprobs)\n",
    "    # build the objective function over perception variables \n",
    "    objective_function = sum(logprobs[idx][v] for idx,v in np.ndenumerate(perception_variables))\n",
    "    model.maximize(objective_function)\n",
    "    \n",
    "    # channeling constraints to link decision variables to perception variables\n",
    "    # perception variable is either 'empty' or matches grid symbol\n",
    "    model+= [(perception_variables != 0).implies(decision_variables == perception_variables)]\n",
    "    # keep track of perception variables as well \n",
    "    visual_sudoku_problem['perception'] = perception_variables\n",
    "    return visual_sudoku_problem\n",
    "\n",
    "def solve_visual_sudoku_full_image(visual_sudoku_problem, solver_params=dict()):\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    s = CPM_ortools(model)\n",
    "    if s.solve(**solver_params):\n",
    "        results = {\n",
    "            'solution':dvars.value(),\n",
    "            'perception':pvars.value()\n",
    "        }\n",
    "    else:\n",
    "        # in case of infeasibility, nan\n",
    "        results = {\n",
    "            'solution':np.full_like(dvars.value(), np.nan),\n",
    "            'perception':np.full_like(dvars.value(), np.nan)\n",
    "        }\n",
    "    results['status'] = np.asarray(s.cpm_status.exitstatus.value)\n",
    "    results['runtime'] = np.asarray(s.cpm_status.runtime)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd48e01-d16a-4965-91b6-fec3680c354c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7000140-f6a2-443b-af95-fbc5d2c813a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perception_leads_to_unique_solution(perception, solution):\n",
    "    sudoku_mod = get_sudoku_model(n=9)\n",
    "    vars = sudoku_mod[\"variables\"]\n",
    "    model = sudoku_mod[\"model\"]\n",
    "    model += ~ cp.all((vars == solution).flatten())\n",
    "    is_unique = not solve_sudoku_again(model, vars, perception)\n",
    "    return is_unique\n",
    "\n",
    "\n",
    "def solve_sudoku_again(model, dvars, instance):\n",
    "    # use another object for solving\n",
    "    newmodel = cp.Model(model.constraints) \n",
    "    # set given clues\n",
    "    newmodel += cp.all(instance[instance>0] == dvars[instance>0])\n",
    "    return newmodel.solve()\n",
    "\n",
    "\n",
    "def solve_visual_sudoku_higher_order(visual_sudoku_problem, solver_params=dict(), max_iter=10):\n",
    "    #Write a loop repeating following steps:\n",
    "    # while results['solution'] is not unique or iteration < max_iter:\n",
    "    #   add nogood to the vizsudoku model\n",
    "    #   solve again\n",
    "    #   iteration += 1\n",
    "    \n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    results = solve_visual_sudoku_full_image(visual_sudoku_problem, solver_params)\n",
    "    #print(\"first \", results)\n",
    "    iteration = 0\n",
    "    \n",
    "    while (not perception_leads_to_unique_solution(pvars.value(), dvars.value())) and (iteration < max_iter):\n",
    "      # add nogood to the vizsudoku model\n",
    "      #model += [~cp.all((perception_variables ==  perception_variables.value())) ]\n",
    "      model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "      results = solve_visual_sudoku_full_image(visual_sudoku_problem, solver_params)\n",
    "      iteration += 1\n",
    "    if iteration >= max_iter:\n",
    "        print(\"max iter exceeded at solving for unique solution\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b42347-4eea-4b61-9b1e-5e27911ebe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_unique_solution(visual_sudoku, cnn):\n",
    "    # convolutional neural network predictions \n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    \n",
    "    # visual sudoku cp model\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    \n",
    "    # solve \n",
    "    #results = solve_visual_sudoku_full_image(visual_sudoku_problem)\n",
    "    results = solve_visual_sudoku_higher_order(visual_sudoku_problem)\n",
    "    return results\n",
    "\n",
    "\n",
    "def inference_simple(visual_sudoku, cnn):\n",
    "    # No check for unique solution\n",
    "    # convolutional neural network predictions \n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    \n",
    "    # visual sudoku cp model\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    \n",
    "    # solve \n",
    "    results = solve_visual_sudoku_full_image(visual_sudoku_problem)\n",
    "    return results\n",
    "\n",
    "\n",
    "def top_k_inference_unique_solution(visual_sudoku, cnn, k=1):\n",
    "    return_list = []\n",
    "    # convolutional neural network predictions \n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    # visual sudoku cp model\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    for i in range(0, k):\n",
    "        results = solve_visual_sudoku_higher_order(visual_sudoku_problem)\n",
    "        return_list.append(results)\n",
    "        model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "        #model += ~ cp.all((dvars == results['solution']).flatten()) # new addition\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def top_k_inference_simple(visual_sudoku, cnn, k=1):\n",
    "    return_list = []\n",
    "    # convolutional neural network predictions \n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    # visual sudoku cp model\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    for i in range(0, k):\n",
    "        results = solve_visual_sudoku_full_image(visual_sudoku_problem)\n",
    "        return_list.append(results)\n",
    "        model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "        #model += ~ cp.all((dvars == results['solution']).flatten()) # new addition\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def top_k_inference_simple_fast(visual_sudoku, cnn, k=1, with_hint=False):\n",
    "    return_list = []\n",
    "    # convolutional neural network predictions \n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    # visual sudoku cp model\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    solver_params=dict()\n",
    "    # solve\n",
    "    counter = 0\n",
    "    s = SolverLookup.get(\"ortools\", model) # faster on a solver interface directly\n",
    "    while s.solve():\n",
    "        counter += 1\n",
    "        results = {\n",
    "                'solution':dvars.value(),\n",
    "                'perception':pvars.value()\n",
    "        }\n",
    "        results['status'] = np.asarray(s.cpm_status.exitstatus.value)\n",
    "        results['runtime'] = np.asarray(s.cpm_status.runtime)\n",
    "        return_list.append(results)\n",
    "        # check counter\n",
    "        if counter==k:\n",
    "            break\n",
    "        # alter constraints\n",
    "        s += ~ cp.all((pvars == pvars.value()).flatten())\n",
    "        # give previous solution as hint\n",
    "        if with_hint:\n",
    "            s.solution_hint(pvars.flatten(), pvars.value().flatten())\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def top_k_inference_simple_fast_alternative(visual_sudoku, cnn, k=1):\n",
    "    return_list = []\n",
    "    # convolutional neural network predictions \n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    # visual sudoku cp model\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    solver_params=dict()\n",
    "    for i in range(0, k):\n",
    "        # Solve\n",
    "        s = CPM_ortools(model)\n",
    "        if s.solve(**solver_params):\n",
    "            results = {\n",
    "                'solution':dvars.value(),\n",
    "                'perception':pvars.value()\n",
    "            }\n",
    "        else:\n",
    "            # in case of infeasibility, nan\n",
    "            results = {\n",
    "                'solution':np.full_like(dvars.value(), np.nan),\n",
    "                'perception':np.full_like(dvars.value(), np.nan)\n",
    "            }\n",
    "        results['status'] = np.asarray(s.cpm_status.exitstatus.value)\n",
    "        results['runtime'] = np.asarray(s.cpm_status.runtime)\n",
    "        return_list.append(results)\n",
    "        # Add new constraint\n",
    "        model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "        # give previous solution as hint\n",
    "        s.solution_hint(pvars.flatten(), pvars.value().flatten())\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d145682-a2bc-4598-b917-16a254a4bb14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Symbolic feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c26a91-32bf-429b-a129-e789ba8e2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def find_index_of_element_in_arrays(arrays, t):\n",
    "    \"\"\"\n",
    "    Find the index of an element with value t across multiple NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        arrays (list of numpy.ndarray): List of NumPy arrays.\n",
    "        t (int or float): Value to find across the arrays.\n",
    "\n",
    "    Returns:\n",
    "        index (tuple or None): Index of the element in the arrays, or None if not found.\n",
    "    \"\"\"\n",
    "    # Check if all arrays have the same shape\n",
    "    shapes = [arr.shape for arr in arrays]\n",
    "    if len(set(shapes)) != 1:\n",
    "        raise ValueError(\"All arrays must have the same shape\")\n",
    "\n",
    "    # Stack arrays along a new axis\n",
    "    stacked_array = np.stack(arrays)\n",
    "\n",
    "    # Check if t exists in all arrays\n",
    "    presence = np.all(stacked_array == t, axis=0)\n",
    "\n",
    "    # If t is present in all arrays, find its index\n",
    "    if np.any(presence):\n",
    "        index = np.where(presence)\n",
    "        return (index[0][0], index[1][0])  # Return only the index of the first occurrence\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def inference_with_feedback(visual_sudoku, cnn, sln, top_k_boards=5):\n",
    "    # convolutional neural network predictions \n",
    "    original_ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    ml_predictions = copy.deepcopy(original_ml_predictions)\n",
    "    \n",
    "    # call the solver\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    board_list = []\n",
    "    complete_board_list = []\n",
    "    for i in range(0, top_k_boards):\n",
    "        results = solve_visual_sudoku_higher_order(visual_sudoku_problem)\n",
    "        board_list.append(results)\n",
    "        complete_board_list.append(results)\n",
    "        model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "        #model += ~ cp.all((dvars == results['solution']).flatten()) # new addition\n",
    "    top_boards = [board[\"perception\"] for board in board_list]\n",
    "\n",
    "    # refine probabilities\n",
    "    for i in range(0, 9):\n",
    "        for j in range(0, 9):\n",
    "            initial_probabilities = ml_predictions[i, j]\n",
    "            partial_evidence = [board[i, j] for board in top_boards]\n",
    "            if (len(set(partial_evidence)) > 1) and (len(set(partial_evidence)) < 10):\n",
    "                # Construct complete input for the sln\n",
    "                complete_input = torch.zeros(1, 11, 1, 28, 28)\n",
    "                complete_input[0, 0] = visual_sudoku[i, j]\n",
    "                for k in range(0, 10):\n",
    "                    index = find_index_of_element_in_arrays(top_boards, k)\n",
    "                    if index is not None:\n",
    "                        complete_input[0, k+1] = visual_sudoku[index[0], index[1]]\n",
    "                with torch.no_grad():\n",
    "                    refined_probability = sln(complete_input)[0]\n",
    "                    refined_probability = torch.nn.functional.softmax(refined_probability, dim=0).numpy()\n",
    "                ml_predictions[i, j] = refined_probability\n",
    "\n",
    "    # Max top board\n",
    "    choice = 0\n",
    "    top_score = 0\n",
    "    new_scores = []\n",
    "    old_scores = []\n",
    "    for i, board in enumerate(top_boards):\n",
    "        score = 1\n",
    "        old_score = 1\n",
    "        for row in range(0,9):\n",
    "            for col in range(0,9):\n",
    "                score *= ml_predictions[row, col, board[row, col]]\n",
    "                old_score *= original_ml_predictions[row, col, board[row, col]]\n",
    "        new_scores.append(score)\n",
    "        old_scores.append(old_score)\n",
    "        if score > top_score:\n",
    "            top_score = score\n",
    "            choice = i \n",
    "    \n",
    "    return complete_board_list[choice]\n",
    "\n",
    "\n",
    "def inference_with_feedback_double_cnn(visual_sudoku, cnn, sln, top_k_boards=5):\n",
    "    # convolutional neural network predictions \n",
    "    original_ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    ml_predictions = copy.deepcopy(original_ml_predictions)\n",
    "    \n",
    "    # call the solver\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    board_list = []\n",
    "    complete_board_list = []\n",
    "    for i in range(0, top_k_boards):\n",
    "        results = solve_visual_sudoku_higher_order(visual_sudoku_problem)\n",
    "        board_list.append(results)\n",
    "        complete_board_list.append(results)\n",
    "        model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "        #model += ~ cp.all((dvars == results['solution']).flatten()) # new addition\n",
    "    top_boards = [board[\"perception\"] for board in board_list]\n",
    "\n",
    "    # refine probabilities\n",
    "    for i in range(0, 9):\n",
    "        for j in range(0, 9):\n",
    "            initial_probabilities = ml_predictions[i, j]\n",
    "            partial_evidence = [board[i, j] for board in top_boards]\n",
    "            if (len(set(partial_evidence)) > 1) and (len(set(partial_evidence)) < 10):\n",
    "                with torch.no_grad():\n",
    "                    refined_probability = sln(visual_sudoku[i, j])[0]\n",
    "                    refined_probability = torch.nn.functional.softmax(refined_probability, dim=0)\n",
    "                ml_predictions[i, j] = refined_probability\n",
    "\n",
    "    # Max top board\n",
    "    choice = 0\n",
    "    top_score = 0\n",
    "    new_scores = []\n",
    "    old_scores = []\n",
    "    for i, board in enumerate(top_boards):\n",
    "        score = 1\n",
    "        old_score = 1\n",
    "        for row in range(0,9):\n",
    "            for col in range(0,9):\n",
    "                score *= ml_predictions[row, col, board[row, col]].item()\n",
    "                old_score *= original_ml_predictions[row, col, board[row, col]].item()\n",
    "        new_scores.append(score)\n",
    "        old_scores.append(old_score)\n",
    "        if score > top_score:\n",
    "            top_score = score\n",
    "            choice = i\n",
    "    \n",
    "    return complete_board_list[choice]\n",
    "\n",
    "\n",
    "def inference_with_feedback_double_cnn_simple(visual_sudoku, cnn, sln, top_k_boards=5):\n",
    "    # convolutional neural network predictions \n",
    "    original_ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    ml_predictions = copy.deepcopy(original_ml_predictions)\n",
    "    \n",
    "    # call the solver\n",
    "    visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "    model = visual_sudoku_problem['model']\n",
    "    dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "    board_list = []\n",
    "    complete_board_list = []\n",
    "    for i in range(0, top_k_boards):\n",
    "        results = solve_visual_sudoku_full_image(visual_sudoku_problem)\n",
    "        board_list.append(results)\n",
    "        complete_board_list.append(results)\n",
    "        model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "        #model += ~ cp.all((dvars == results['solution']).flatten()) # new addition\n",
    "    top_boards = [board[\"perception\"] for board in board_list]\n",
    "\n",
    "    # refine probabilities\n",
    "    for i in range(0, 9):\n",
    "        for j in range(0, 9):\n",
    "            initial_probabilities = ml_predictions[i, j]\n",
    "            partial_evidence = [board[i, j] for board in top_boards]\n",
    "            if (len(set(partial_evidence)) > 1) and (len(set(partial_evidence)) < 10):\n",
    "                with torch.no_grad():\n",
    "                    refined_probability = sln(visual_sudoku[i, j])[0]\n",
    "                    refined_probability = torch.nn.functional.softmax(refined_probability, dim=0)\n",
    "                ml_predictions[i, j] = refined_probability\n",
    "\n",
    "    # Max top board\n",
    "    choice = 0\n",
    "    top_score = 0\n",
    "    new_scores = []\n",
    "    old_scores = []\n",
    "    for i, board in enumerate(top_boards):\n",
    "        score = 1\n",
    "        old_score = 1\n",
    "        for row in range(0,9):\n",
    "            for col in range(0,9):\n",
    "                score *= ml_predictions[row, col, board[row, col]].item()\n",
    "                old_score *= original_ml_predictions[row, col, board[row, col]].item()\n",
    "        new_scores.append(score)\n",
    "        old_scores.append(old_score)\n",
    "        if score > top_score:\n",
    "            top_score = score\n",
    "            choice = i\n",
    "    \n",
    "    return complete_board_list[choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff3f34-2d66-4747-9a32-4c8f661acdeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Finetune SLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401447ef-7bdd-4ea5-b4e8-dd17b717f4a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make train list\n",
    "\n",
    "trained_cnn = PieceByPieceBoardCNN10()\n",
    "trained_cnn.load_state_dict(torch.load('PieceByPieceBoardCNN10_SPOP_acc_0.966_2.pth'))\n",
    "trained_cnn.eval()\n",
    "\n",
    "train_list = []\n",
    "\n",
    "for index in range(0, 3000):\n",
    "    numerical_sudoku = numerical_train_sudokus[index].numpy()\n",
    "    visual_sudoku = visual_train_sudokus[index]\n",
    "    \n",
    "    top_boards = top_k_inference_simple(visual_sudoku, trained_cnn, k=3)\n",
    "    diff = (top_boards[0][\"perception\"]-top_boards[1][\"perception\"]) + (top_boards[0][\"perception\"]-top_boards[2][\"perception\"]) + (top_boards[1][\"perception\"]-top_boards[2][\"perception\"])\n",
    "    true_indices = np.where(diff!=0)\n",
    "\n",
    "    first_board = [top_boards[0][\"perception\"][true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "    second_board = [top_boards[1][\"perception\"][true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "    third_board = [top_boards[2][\"perception\"][true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "    correct_board = [numerical_sudoku[true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "\n",
    "    for k in range(0, len(first_board)):\n",
    "        element = [first_board[k], second_board[k], third_board[k], correct_board[k], index, true_indices[0][k], true_indices[1][k]]\n",
    "        train_list.append(element)\n",
    "\n",
    "    if index % 20 == 19:\n",
    "        print(f'Board {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a39ad-d10c-4312-a2ae-54c49a03ddb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make validation list\n",
    "\n",
    "trained_cnn = PieceByPieceBoardCNN10()\n",
    "trained_cnn.load_state_dict(torch.load('PieceByPieceBoardCNN10_SPOP_acc_0.966_2.pth'))\n",
    "trained_cnn.eval()\n",
    "\n",
    "validation_list = []\n",
    "\n",
    "for index in range(0, 500):\n",
    "    numerical_sudoku = numerical_validation_sudokus[index].numpy()\n",
    "    visual_sudoku = visual_validation_sudokus[index]\n",
    "    \n",
    "    top_boards = top_k_inference_simple(visual_sudoku, trained_cnn, k=3)\n",
    "    diff = (top_boards[0][\"perception\"]-top_boards[1][\"perception\"]) + (top_boards[0][\"perception\"]-top_boards[2][\"perception\"]) + (top_boards[1][\"perception\"]-top_boards[2][\"perception\"])\n",
    "    true_indices = np.where(diff!=0)\n",
    "\n",
    "    first_board = [top_boards[0][\"perception\"][true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "    second_board = [top_boards[1][\"perception\"][true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "    third_board = [top_boards[2][\"perception\"][true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "    correct_board = [numerical_sudoku[true_indices[0][i], true_indices[1][i]] for i in range(0, len(true_indices[0]))]\n",
    "\n",
    "    board_elements = []\n",
    "    for k in range(0, len(first_board)):\n",
    "        element = [first_board[k], second_board[k], third_board[k], correct_board[k], index, true_indices[0][k], true_indices[1][k]]\n",
    "        board_elements.append(element)\n",
    "    validation_list.append(board_elements)\n",
    "\n",
    "    if index % 20 == 19:\n",
    "        print(f'Board {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad4c87-dde6-432e-a35b-cfe220765889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate board accuracy when using the SLN and symbolic feedback on validation boards,\n",
    "# validation list has been computed earlier with the CNN\n",
    "\n",
    "def print_acc_on_validation_boards(sln, validation_list):\n",
    "    correct = 0\n",
    "    total_digits = 0\n",
    "    loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for boards in validation_list:\n",
    "        correct_prob = 1.0\n",
    "        prob1 = 1.0\n",
    "        prob2 = 1.0\n",
    "        prob3 = 1.0\n",
    "        for k in range(0, len(boards)):\n",
    "            element = [boards[k][0], boards[k][1], boards[k][2], boards[k][3], boards[k][4], boards[k][5], boards[k][6]]\n",
    "            total_digits += 1\n",
    "            \n",
    "            index = element[4]\n",
    "            visual_sudoku = visual_validation_sudokus[index]\n",
    "            \n",
    "            output = sln(visual_sudoku[element[5], element[6]])[0]\n",
    "            pred = torch.nn.functional.softmax(output, dim=0).detach().numpy()\n",
    "            loss += criterion(output.unsqueeze(0), torch.tensor([element[3]]).long())\n",
    "    \n",
    "            correct_prob *= pred[element[3]]\n",
    "            prob1 *= pred[element[0]]\n",
    "            prob2 *= pred[element[1]]\n",
    "            prob3 *= pred[element[2]]\n",
    "            \n",
    "        if np.max([prob1, prob2, prob3]) == correct_prob:\n",
    "            correct += 1\n",
    "\n",
    "    print(\"##### Results validation #####\")\n",
    "    print(\"Accuracy: \", correct/len(validation_list))\n",
    "    print(\"Loss: \", loss.item()/len(validation_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc3436-0b12-4f45-999b-c8eb56ab4953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finetune SLN using the train list\n",
    "\n",
    "trained_sln = TinyLeNet()\n",
    "trained_sln.load_state_dict(torch.load('TinyLeNet_0.959.pth'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(trained_sln.parameters(), lr=0.0001)\n",
    "running_loss = 0\n",
    "\n",
    "for i, element in enumerate(train_list, 0):\n",
    "    index = element[4]\n",
    "    numerical_sudoku = numerical_train_sudokus[index].numpy()\n",
    "    visual_sudoku = visual_train_sudokus[index]\n",
    "    \n",
    "    # Train\n",
    "    optimizer.zero_grad()\n",
    "    output = trained_sln(visual_sudoku[element[5], element[6]])[0]\n",
    "    label = torch.Tensor([element[3]]).long()\n",
    "    loss = criterion(output.unsqueeze(0), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(\"------------------------------\")\n",
    "        print(f'Board {i}, Train Loss: {running_loss/100:.4f}')\n",
    "        running_loss = 0.0\n",
    "        print_acc_on_validation_boards(trained_sln, validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed74f6-b97f-4f14-afea-c7a1cc0cf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "# Manual loading\n",
    "sln = TinyLeNet()\n",
    "sln.load_state_dict(torch.load('TinyLeNet_0.959.pth'))\n",
    "\n",
    "print_acc_on_validation_boards(sln, validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3677e-5676-4bfc-8628-f897e4c7fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual saving\n",
    "\n",
    "torch.save(trained_sln.state_dict(), 'TinyLeNet_SLN.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ad6d5-09d2-4f74-b300-d69c7bd7bc42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DFL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80464eaa-e945-4a49-b421-e0bd978e5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optModel\n",
    "\n",
    "import pyepo\n",
    "from pyepo.model.opt import optModel\n",
    "import cpmpy as cp\n",
    "from cpmpy.solvers import CPM_ortools\n",
    "import copy\n",
    "\n",
    "\n",
    "def mlPredictionsToLogprobs(ml_predictions, precision=1e-5):\n",
    "    ml_predictions = torch.tensor(ml_predictions)\n",
    "    # Convert predictions to logspace using torch\n",
    "    logprobs = torch.log(torch.maximum(ml_predictions, torch.tensor(precision, dtype=ml_predictions.dtype)))\n",
    "    # cp solver requires integer values\n",
    "    logprobs = (logprobs / precision).to(torch.int32)\n",
    "    #logprobs = (logprobs / precision)\n",
    "    return logprobs\n",
    "    \n",
    "\n",
    "def transformToOneHot(input_array):\n",
    "    # Convert the input array to a tensor\n",
    "    input_tensor = torch.tensor(input_array)\n",
    "    # Use one_hot function to create one-hot encoded tensor\n",
    "    one_hot_array = torch.nn.functional.one_hot(input_tensor, num_classes=10)\n",
    "    # Convert the one-hot encoded tensor to integer type\n",
    "    #one_hot_array = one_hot_array.to(torch.int)\n",
    "    one_hot_array = one_hot_array.to(torch.float32)\n",
    "    return one_hot_array\n",
    "\n",
    "\n",
    "class sudokuModel(optModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grid (tuple): size of grid network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logprobs = None\n",
    "        self.modelSense = -1   # Maximize objective function?\n",
    "\n",
    "    def _getModel(self):\n",
    "        \"\"\"\n",
    "        A method to build model\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimization model and variables\n",
    "        \"\"\"\n",
    "        n = 9\n",
    "        b = np.sqrt(n).astype(int)\n",
    "        cells = cp.IntVar(1, n, shape=(n,n))\n",
    "    \n",
    "        # plain sudoku model\n",
    "        m = cp.Model(\n",
    "            [cp.alldifferent(row) for row in cells],\n",
    "            [cp.alldifferent(col) for col in cells.T],\n",
    "            [cp.alldifferent(cells[i:i + b, j:j + b])\n",
    "                for i in range(0, n, b) for j in range(0, n, b)],\n",
    "        )\n",
    "        return_model = {\n",
    "            'model':m,\n",
    "            'variables':cells\n",
    "        }\n",
    "        return return_model, cells\n",
    "\n",
    "    def _getFreshModel(self):\n",
    "        n = 9\n",
    "        b = np.sqrt(n).astype(int)\n",
    "        cells = cp.IntVar(1, n, shape=(n,n))\n",
    "        # plain sudoku model\n",
    "        m = cp.Model(\n",
    "            [cp.alldifferent(row) for row in cells],\n",
    "            [cp.alldifferent(col) for col in cells.T],\n",
    "            [cp.alldifferent(cells[i:i + b, j:j + b])\n",
    "                for i in range(0, n, b) for j in range(0, n, b)],\n",
    "        )\n",
    "        fresh_model = {\n",
    "            'model':m,\n",
    "            'variables':cells\n",
    "        }\n",
    "        return fresh_model\n",
    "\n",
    "    def setObj(self, c):\n",
    "        \"\"\"\n",
    "        A method to set objective function\n",
    "\n",
    "        Args:\n",
    "            c : lobprobs as a numpy array\n",
    "        \"\"\"\n",
    "        # print(\"  c   \")\n",
    "        # print(c)\n",
    "        self.probs = c.reshape(9, 9, 10)\n",
    "        self.logprobs = mlPredictionsToLogprobs(self.probs).numpy()\n",
    "        \n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        A method to solve model, call strictly after calling setObj\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (list) and objective value (float)\n",
    "        \"\"\"\n",
    "        # Obtain a fresh model\n",
    "        visual_sudoku_problem = self._getFreshModel()\n",
    "        model = visual_sudoku_problem['model']\n",
    "        decision_variables = visual_sudoku_problem['variables']\n",
    "        # introduce a layer of 'perception' variables, as an interface\n",
    "        # between the solver and the ml network\n",
    "        # their domain is [0, ..., 9], with 0 acting as the 'empty' symbol\n",
    "        perception_variables = cp.intvar(0, 9, shape=decision_variables.shape, name='perception')\n",
    "        # Use the latest logprobs\n",
    "        logprobs = cp.cpm_array(self.logprobs)\n",
    "        visual_sudoku_problem['logprobs'] = logprobs\n",
    "        # build the objective function over perception variables \n",
    "        objective_function = sum(logprobs[idx][v] for idx,v in np.ndenumerate(perception_variables))\n",
    "        model.maximize(objective_function)\n",
    "        # channeling constraints to link decision variables to perception variables\n",
    "        # perception variable is either 'empty' or matches grid symbol\n",
    "        model+= [(perception_variables != 0).implies(decision_variables == perception_variables)]\n",
    "        # keep track of perception variables as well \n",
    "        visual_sudoku_problem['perception'] = perception_variables\n",
    "\n",
    "        # Solve\n",
    "        solver_params=dict()\n",
    "        dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "        s = CPM_ortools(model)\n",
    "        if s.solve(**solver_params):\n",
    "            results = {\n",
    "                'solution':dvars.value(),\n",
    "                'perception':pvars.value()\n",
    "            }\n",
    "        else:\n",
    "            # in case of infeasibility, nan\n",
    "            results = {\n",
    "                'solution':np.full_like(dvars.value(), np.nan),\n",
    "                'perception':np.full_like(dvars.value(), np.nan)\n",
    "            }\n",
    "        results['status'] = np.asarray(s.cpm_status.exitstatus.value)\n",
    "        results['runtime'] = np.asarray(s.cpm_status.runtime)\n",
    "\n",
    "        # Compute value objective function\n",
    "        #logprobs = visual_sudoku_problem['logprobs']\n",
    "        if results['perception'][0][0] is not None:\n",
    "            obj = sum(self.probs[idx][v] for idx,v in np.ndenumerate(pvars.value()))\n",
    "        else:\n",
    "            # In case of no solution found\n",
    "            raise Exception(\"No solution found for the sudoku!\")\n",
    "        \n",
    "        return transformToOneHot(results['perception']).view(810), torch.tensor(obj)\n",
    "\n",
    "\n",
    "\n",
    "class sudokuModelWithSymbolicFeedback(optModel):\n",
    "\n",
    "    def __init__(self, sln):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grid (tuple): size of grid network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logprobs = None\n",
    "        self.modelSense = -1   # Maximize objective function?\n",
    "        self.sln = sln\n",
    "        self.differing_digit_indices = None\n",
    "        self.sln_predictions = None\n",
    "\n",
    "    def _getModel(self):\n",
    "        \"\"\"\n",
    "        A method to build model\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimization model and variables\n",
    "        \"\"\"\n",
    "        n = 9\n",
    "        b = np.sqrt(n).astype(int)\n",
    "        cells = cp.IntVar(1, n, shape=(n,n))\n",
    "    \n",
    "        # plain sudoku model\n",
    "        m = cp.Model(\n",
    "            [cp.alldifferent(row) for row in cells],\n",
    "            [cp.alldifferent(col) for col in cells.T],\n",
    "            [cp.alldifferent(cells[i:i + b, j:j + b])\n",
    "                for i in range(0, n, b) for j in range(0, n, b)],\n",
    "        )\n",
    "        return_model = {\n",
    "            'model':m,\n",
    "            'variables':cells\n",
    "        }\n",
    "        return return_model, cells\n",
    "\n",
    "    def _getDifferingDigitIndices(self):\n",
    "        if self.differing_digit_indices == None:\n",
    "            raise Exception(\"Differing digits not yet instantiated, first solve for a visual sudoku!\")\n",
    "        else:\n",
    "            return self.differing_digit_indices\n",
    "\n",
    "    def _getFreshModel(self):\n",
    "        n = 9\n",
    "        b = np.sqrt(n).astype(int)\n",
    "        cells = cp.IntVar(1, n, shape=(n,n))\n",
    "        # plain sudoku model\n",
    "        m = cp.Model(\n",
    "            [cp.alldifferent(row) for row in cells],\n",
    "            [cp.alldifferent(col) for col in cells.T],\n",
    "            [cp.alldifferent(cells[i:i + b, j:j + b])\n",
    "                for i in range(0, n, b) for j in range(0, n, b)],\n",
    "        )\n",
    "        fresh_model = {\n",
    "            'model':m,\n",
    "            'variables':cells\n",
    "        }\n",
    "        return fresh_model\n",
    "\n",
    "    def setObj(self, c):\n",
    "        \"\"\"\n",
    "        A method to set objective function\n",
    "\n",
    "        Args:\n",
    "            c : lobprobs as a numpy array\n",
    "        \"\"\"\n",
    "        # print(\"  c   \")\n",
    "        # print(c)\n",
    "        self.probs = c.reshape(9, 9, 10)\n",
    "        self.logprobs = mlPredictionsToLogprobs(self.probs).numpy()\n",
    "\n",
    "    def setSLNPredictions(self, ml_predictions):\n",
    "        self.sln_predictions = ml_predictions\n",
    "        \n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        A method to solve model, call strictly after calling setObj\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (list) and objective value (float)\n",
    "        \"\"\"\n",
    "        # convolutional neural network predictions \n",
    "        original_ml_predictions = self.probs\n",
    "        ml_predictions = copy.deepcopy(original_ml_predictions)\n",
    "        \n",
    "        # call the solver\n",
    "        visual_sudoku_problem = get_visual_sudoku_full_image_model(ml_predictions)\n",
    "        model = visual_sudoku_problem['model']\n",
    "        dvars, pvars = visual_sudoku_problem['variables'], visual_sudoku_problem['perception']\n",
    "        board_list = []\n",
    "        complete_board_list = []\n",
    "        top_k_boards = 3\n",
    "        for i in range(0, top_k_boards):\n",
    "            results = solve_visual_sudoku_full_image(visual_sudoku_problem)\n",
    "            board_list.append(results)\n",
    "            complete_board_list.append(results)\n",
    "            model += ~ cp.all((pvars == results['perception']).flatten())\n",
    "            #model += ~ cp.all((dvars == results['solution']).flatten()) # new addition\n",
    "        top_boards = [board[\"perception\"] for board in board_list]\n",
    "\n",
    "        # check if you can use sln predictions\n",
    "        if self.sln_predictions != None:\n",
    "    \n",
    "            # refine probabilities\n",
    "            differing_digit_indices = []\n",
    "            for i in range(0, 9):\n",
    "                for j in range(0, 9):\n",
    "                    partial_evidence = [board[i, j] for board in top_boards]\n",
    "                    if len(set(partial_evidence)) > 1:\n",
    "                        differing_digit_indices.append((i,j))\n",
    "                        with torch.no_grad():\n",
    "                            #refined_probability = self.sln(visual_sudoku[i, j])[0]\n",
    "                            #refined_probability = torch.tensor(ml_predictions[i, j])\n",
    "                            #refined_probability = torch.nn.functional.softmax(refined_probability, dim=0)\n",
    "                            refined_probability = self.sln_predictions[i, j]\n",
    "                        ml_predictions[i, j] = refined_probability\n",
    "            # Set the indices\n",
    "            self.differing_digit_indices = differing_digit_indices\n",
    "        \n",
    "            # Max top board\n",
    "            choice = 0\n",
    "            top_score = 0\n",
    "            new_scores = []\n",
    "            old_scores = []\n",
    "            for i, board in enumerate(top_boards):\n",
    "                score = 1\n",
    "                old_score = 1\n",
    "                for row in range(0,9):\n",
    "                    for col in range(0,9):\n",
    "                        score *= ml_predictions[row, col, board[row, col]].item()\n",
    "                        old_score *= original_ml_predictions[row, col, board[row, col]].item()\n",
    "                new_scores.append(score)\n",
    "                old_scores.append(old_score)\n",
    "                if score > top_score:\n",
    "                    top_score = score\n",
    "                    choice = i\n",
    "            results = complete_board_list[choice]\n",
    "\n",
    "        else:\n",
    "            results = board_list[0]\n",
    "        \n",
    "        # Compute value objective function\n",
    "        if results['perception'][0][0] is not None:\n",
    "            #obj = sum(self.probs[idx][v] for idx,v in np.ndenumerate(pvars.value()))\n",
    "            obj = sum(self.probs[idx][v] for idx,v in np.ndenumerate(results[\"perception\"]))\n",
    "        else:\n",
    "            # In case of no solution found\n",
    "            raise Exception(\"No solution found for the sudoku!\")\n",
    "        \n",
    "        return transformToOneHot(results['perception']).view(810), torch.tensor(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3920b-c816-47dd-869b-ec722e2db056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "\n",
    "\n",
    "class CustomSudokuDatasetDFL(Dataset):\n",
    "    def __init__(self, image_dataset, label_dataset):\n",
    "        self.image_data = image_dataset\n",
    "        self.label_data = label_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input features x\n",
    "        x = self.image_data[idx]\n",
    "        # true optimal solution w \n",
    "        w = transformToOneHot(self.label_data[idx]).view(810)\n",
    "        # true costs c (true probs for the images)\n",
    "        c = w\n",
    "        # true optimal objective value z is always 81\n",
    "        z = 81. * torch.ones(1)\n",
    "        \n",
    "        return x, c, w, z\n",
    "\n",
    "\n",
    "# Make datasets\n",
    "trainset = CustomSudokuDatasetDFL(visual_train_sudokus[0:20000], numerical_train_sudokus[0:20000])\n",
    "validationset = CustomSudokuDatasetDFL(visual_validation_sudokus[0:10], numerical_validation_sudokus[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7b99d-71a8-458e-8de6-1d128c1f2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you can accelerate training\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    device = mps_device\n",
    "    print(\"Device: \", device)\n",
    "    #x = torch.ones(1, device=mps_device)\n",
    "    #print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196589c2-bf17-41e4-8dcf-f9a636021266",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without SLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc7cc2-6899-468d-aad2-cbbedb93882d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training without SLN, SPO+\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# init prediction model\n",
    "predmodel = PieceByPieceBoardCNN10(use_softmax=True).to(device)\n",
    "predmodel.load_state_dict(torch.load('PieceByPieceBoardCNN10_acc_0.966.pth'))\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(predmodel.parameters(), lr=1e-7)\n",
    "# init optimization model\n",
    "optmodel = sudokuModel()\n",
    "# init SPO+ loss\n",
    "spop = pyepo.func.SPOPlus(optmodel, processes=5)\n",
    "regret_list = []\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False)\n",
    "num_epochs = 1\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        # forward pass\n",
    "        predictions = predmodel(x)\n",
    "        #predictions = predmodel(x).view(x.shape[0],9,9,10)\n",
    "        # Apply softmax to make probability distributions from logits\n",
    "        #predictions = torch.nn.functional.softmax(predictions, dim=len(predictions.shape)-1)\n",
    "        #predictions = predictions.view(x.shape[0],810)\n",
    "        loss = spop(predictions, c, w, z, reduction=\"mean\")\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch {:2},  Batch: {:2},  Loss: {:}\".format(epoch+1, i+1, loss.item()))\n",
    "        if i % 25 == 24:\n",
    "            regret = pyepo.metric.regret(predmodel, optmodel, validation_loader)\n",
    "            regret_list.append(regret)\n",
    "            print(\"Regret: \", regret)\n",
    "        if i == 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1881f-889c-4ded-b31d-a63388a10dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make optDataset for Pairwise learning to rank\n",
    "\n",
    "# Define the input features x and true costs c\n",
    "x_train = visual_train_sudokus[30000:40000]\n",
    "c_train = [transformToOneHot(solution).view(810) for solution in numerical_train_sudokus[30000:40000]]\n",
    "# Get optDataset\n",
    "dataset_train = pyepo.data.dataset.optDataset(optmodel, x_train, c_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abaff9c-c0de-476e-9a11-51244bf2f076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training without SLN, Pairwise learning to rank\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# init prediction model\n",
    "#predmodel = PieceByPieceBoardCNN10(use_softmax=True).to(device)\n",
    "#predmodel.load_state_dict(torch.load('PieceByPieceBoardCNN10_pairwise_acc_0.966_2.pth'))\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(predmodel.parameters(), lr=1e-7)\n",
    "# init optimization model\n",
    "optmodel = sudokuModel()\n",
    "# init pairwise learning to rank\n",
    "empty_dataset = pyepo.data.dataset.optDataset(optmodel, visual_train_sudokus[0:100], [transformToOneHot(solution).view(810) for solution in numerical_train_sudokus[0:100]])\n",
    "prltr = pyepo.func.pairwiseLTR(optmodel, processes=4, solve_ratio=0.5, dataset=empty_dataset)\n",
    "regret_list = []\n",
    "# set the other parameters\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        # forward pass\n",
    "        predictions = predmodel(x)\n",
    "        #predictions = predmodel(x).view(x.shape[0],9,9,10)\n",
    "        # Apply softmax to make probability distributions from logits\n",
    "        #predictions = torch.nn.functional.softmax(predictions, dim=len(predictions.shape)-1)\n",
    "        #predictions = predictions.view(x.shape[0],810)\n",
    "        #loss = spop(predictions, c, w, z, reduction=\"mean\")\n",
    "        #loss = pfy(predictions, w)\n",
    "        loss = prltr(predictions, c)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch {:2},  Batch: {:2},  Loss: {:}\".format(epoch+1, i+1, loss.item()))\n",
    "        if i % 25 == 24:\n",
    "            regret = pyepo.metric.regret(predmodel, optmodel, validation_loader)\n",
    "            regret_list.append(regret)\n",
    "            print(\"Regret: \", regret)\n",
    "        # Early stopping\n",
    "        # if i == 200:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48850b-eeb9-48ca-9c1e-b11d270b0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "validationset_evaluate = CustomSudokuDatasetDFL(visual_validation_sudokus[0:500], numerical_validation_sudokus[0:500])\n",
    "validation_loader_evaluate = DataLoader(validationset_evaluate, batch_size=batch_size, shuffle=False)\n",
    "regret = pyepo.metric.regret(predmodel, optmodel, validation_loader_evaluate)\n",
    "print(\"Regret: \", regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b09fe2-f041-4b24-9469-c52d57c0caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual saving\n",
    "\n",
    "torch.save(predmodel.state_dict(), 'PieceByPieceBoardCNN10_pairwise_acc_0.9_3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968c079-811e-44e9-b28a-9404d4a75ba8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With SLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852c31a-7ddd-4b6d-a2f5-48f29d10400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regretForSymoblicFeedback(predmodel, sln, optmodel, dataloader):\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            cp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(cp.shape[0]):\n",
    "            # SLN prediction\n",
    "            visual_sudoku = x[j]\n",
    "            grid_shape = visual_sudoku.shape[:2]\n",
    "            # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "            pred = sln(visual_sudoku.flatten(0,1))\n",
    "            #pred = cnn(visual_sudoku.unsqueeze(0).to(device))\n",
    "            pred = pred.view(9,9,10)\n",
    "            pred = F.softmax(pred, dim=2)\n",
    "            pred = pred.cpu()  # Move the tensor to the CPU\n",
    "            # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "            ml_predictions =  pred.reshape(*grid_shape,10).detach() # reshape as 9x9x10 tensor for easier visualisation\n",
    "            optmodel.setSLNPredictions(ml_predictions)\n",
    "            \n",
    "            # accumulate loss\n",
    "            loss += calRegretSymbolicFeedback(optmodel, cp[j], c[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "def calRegretSymbolicFeedback(optmodel, pred_cost, true_cost, true_obj):\n",
    "    # opt sol for pred cost\n",
    "    optmodel.setObj(pred_cost)\n",
    "    sol, _ = optmodel.solve()\n",
    "    # obj with true cost\n",
    "    obj = np.dot(sol, true_cost)\n",
    "    # loss\n",
    "    if optmodel.modelSense == 1:\n",
    "        loss = obj - true_obj\n",
    "    if optmodel.modelSense == -1:\n",
    "        loss = true_obj - obj\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ff4fc-fc53-4039-913a-8669d1408508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training with SLN\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# init prediction model\n",
    "predmodel = PieceByPieceBoardCNN10(use_softmax=True).to(device)\n",
    "predmodel.load_state_dict(torch.load('PieceByPieceBoardCNN10_acc_0.966.pth'))\n",
    "# init SLN\n",
    "sln = TinyLeNet().to(device)\n",
    "sln.load_state_dict(torch.load('TinyLeNet_SLN_0.955_finetuned_for_PBPCNN10_0.966_2_SPOP.pth'))\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(predmodel.parameters(), lr=1e-7)\n",
    "# init optimization model\n",
    "#optmodel = sudokuModel()\n",
    "optmodel = sudokuModelWithSymbolicFeedback(sln)\n",
    "# init SPO+ loss\n",
    "spop = pyepo.func.SPOPlus(optmodel, processes=1)\n",
    "regret_list = []\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False)\n",
    "num_epochs = 1\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        # forward pass\n",
    "        predictions = predmodel(x)\n",
    "        # SLN prediction\n",
    "        visual_sudoku = x[0]\n",
    "        grid_shape = visual_sudoku.shape[:2]\n",
    "        # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "        pred = sln(visual_sudoku.flatten(0,1))\n",
    "        #pred = cnn(visual_sudoku.unsqueeze(0).to(device))\n",
    "        pred = pred.view(9,9,10)\n",
    "        pred = F.softmax(pred, dim=2)\n",
    "        pred = pred.cpu()  # Move the tensor to the CPU\n",
    "        # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "        ml_predictions =  pred.reshape(*grid_shape,10).detach() # reshape as 9x9x10 tensor for easier visualisation\n",
    "        optmodel.setSLNPredictions(ml_predictions)\n",
    "        \n",
    "        loss = spop(predictions, c, w, z, reduction=\"mean\")\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch {:2},  Batch: {:2},  Loss: {:}\".format(epoch+1, i+1, loss.item()))\n",
    "        if i % 250 == 249:\n",
    "            regret = regretForSymoblicFeedback(predmodel, sln, optmodel, validation_loader)\n",
    "            regret_list.append(regret)\n",
    "            print(\"Regret: \", regret)\n",
    "        # if i == 99:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2844016-a4ed-4aa2-bab8-953b6d9aa6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training with SLN, pairwise\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# init prediction model\n",
    "predmodel = PieceByPieceBoardCNN10(use_softmax=True).to(device)\n",
    "predmodel.load_state_dict(torch.load('PieceByPieceBoardCNN10_pairwise_with_SLN_acc_0.966_2.pth'))\n",
    "# init SLN\n",
    "sln = TinyLeNet().to(device)\n",
    "sln.load_state_dict(torch.load('TinyLeNet_0.959.pth'))\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(predmodel.parameters(), lr=1e-7)\n",
    "# init optimization model\n",
    "optmodel = sudokuModelWithSymbolicFeedback(sln)\n",
    "# init pairwise learning to rank\n",
    "empty_dataset = pyepo.data.dataset.optDataset(optmodel, visual_train_sudokus[0:10], [transformToOneHot(solution).view(810) for solution in numerical_train_sudokus[0:10]])\n",
    "prltr = pyepo.func.pairwiseLTR(optmodel, processes=1, solve_ratio=0.5, dataset=empty_dataset)\n",
    "# init rest\n",
    "regret_list = []\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False)\n",
    "num_epochs = 1\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        # forward pass\n",
    "        predictions = predmodel(x)\n",
    "        # SLN prediction\n",
    "        visual_sudoku = x[0]\n",
    "        grid_shape = visual_sudoku.shape[:2]\n",
    "        # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "        pred = sln(visual_sudoku.flatten(0,1))\n",
    "        #pred = cnn(visual_sudoku.unsqueeze(0).to(device))\n",
    "        pred = pred.view(9,9,10)\n",
    "        pred = F.softmax(pred, dim=2)\n",
    "        pred = pred.cpu()  # Move the tensor to the CPU\n",
    "        # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "        ml_predictions =  pred.reshape(*grid_shape,10).detach() # reshape as 9x9x10 tensor for easier visualisation\n",
    "        optmodel.setSLNPredictions(ml_predictions)\n",
    "        \n",
    "        loss = prltr(predictions, c)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch {:2},  Batch: {:2},  Loss: {:}\".format(epoch+1, i+1, loss.item()))\n",
    "        if i % 250 == 249:\n",
    "            regret = regretForSymoblicFeedback(predmodel, sln, optmodel, validation_loader)\n",
    "            regret_list.append(regret)\n",
    "            print(\"Regret: \", regret)\n",
    "        # if i == 99:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e5f44-f8e8-4859-ada0-4ab07ae8d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predmodel\n",
    "\n",
    "validationset_evaluate = CustomSudokuDatasetDFL(visual_validation_sudokus[0:200], numerical_validation_sudokus[0:200])\n",
    "validation_loader_evaluate = DataLoader(validationset_evaluate, batch_size=batch_size, shuffle=False)\n",
    "regret = regretForSymoblicFeedback(predmodel, sln, optmodel, validation_loader_evaluate)\n",
    "print(\"Regret: \", regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cf58a-59a3-433b-93e2-b00fc9a523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual saving\n",
    "\n",
    "torch.save(predmodel.state_dict(), '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b749ae3-ca9e-40db-9762-720a5d73aced",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train CNN and SLN simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154dd73-922f-4745-b692-913f6c3b0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regretForSymoblicFeedbackWithSLNLoss(predmodel, sln, optmodel, dataloader):\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    sln_loss = 0\n",
    "    optsum = 0\n",
    "    total_boards = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            cp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(cp.shape[0]):\n",
    "            # SLN prediction\n",
    "            visual_sudoku = x[j]\n",
    "            grid_shape = visual_sudoku.shape[:2]\n",
    "            # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "            pred = sln(visual_sudoku.flatten(0,1))\n",
    "            #pred = cnn(visual_sudoku.unsqueeze(0).to(device))\n",
    "            pred = pred.view(9,9,10)\n",
    "            pred = F.softmax(pred, dim=2)\n",
    "            pred = pred.cpu()  # Move the tensor to the CPU\n",
    "            # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "            sln_predictions =  pred.reshape(*grid_shape,10).detach() # reshape as 9x9x10 tensor for easier visualisation\n",
    "            optmodel.setSLNPredictions(sln_predictions)\n",
    "            \n",
    "            # accumulate loss\n",
    "            loss += calRegretSymbolicFeedback(optmodel, cp[j], c[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "\n",
    "            # accumulate sln loss\n",
    "            differing_digit_indices = optmodel._getDifferingDigitIndices()\n",
    "            sln_diff_digits_preds = torch.stack([sln_predictions[index] for index in differing_digit_indices])\n",
    "            diff_digits_labels = torch.stack([w[j].view(9,9,10)[index] for index in differing_digit_indices]).cpu()\n",
    "            sln_loss += sln_criterion(sln_diff_digits_preds, diff_digits_labels) \n",
    "            total_boards += cp.shape[0]\n",
    "            \n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return (loss / (optsum + 1e-7)), (sln_loss / total_boards)\n",
    "\n",
    "def calRegretSymbolicFeedback(optmodel, pred_cost, true_cost, true_obj):\n",
    "    # opt sol for pred cost\n",
    "    optmodel.setObj(pred_cost)\n",
    "    sol, _ = optmodel.solve()\n",
    "    # obj with true cost\n",
    "    obj = np.dot(sol, true_cost)\n",
    "    # loss\n",
    "    if optmodel.modelSense == 1:\n",
    "        loss = obj - true_obj\n",
    "    if optmodel.modelSense == -1:\n",
    "        loss = true_obj - obj\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d639996-12bb-480e-81c3-9065445386fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training CNN and SLN\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# init prediction model\n",
    "predmodel = PieceByPieceBoardCNN10(use_softmax=True).to(device)\n",
    "predmodel.load_state_dict(torch.load('PieceByPieceBoardCNN10_acc_0.966.pth'))\n",
    "\n",
    "# init SLN\n",
    "sln = TinyLeNet().to(device)\n",
    "sln.load_state_dict(torch.load('TinyLeNet_0.959.pth'))\n",
    "sln_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizers\n",
    "optimizer = torch.optim.Adam(predmodel.parameters(), lr=1e-7)\n",
    "optimizer_sln = torch.optim.Adam(sln.parameters(), lr=1e-4)\n",
    "\n",
    "# init optimization model\n",
    "optmodel = sudokuModelWithSymbolicFeedback(sln)\n",
    "\n",
    "# init SPO+ loss\n",
    "spop = pyepo.func.SPOPlus(optmodel, processes=1)\n",
    "\n",
    "# init other\n",
    "regret_list = []\n",
    "batch_size = 1  # Has to remain 1\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False)\n",
    "num_epochs = 1\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        \n",
    "        # forward pass CNN\n",
    "        predictions = predmodel(x)\n",
    "        \n",
    "        # SLN prediction\n",
    "        visual_sudoku = x[0]\n",
    "        grid_shape = visual_sudoku.shape[:2]\n",
    "        # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "        pred = sln(visual_sudoku.flatten(0,1))\n",
    "        pred = pred.view(9,9,10)\n",
    "        pred = F.softmax(pred, dim=2)\n",
    "        pred = pred.cpu()  # Move the tensor to the CPU\n",
    "        # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "        sln_predictions =  pred.reshape(*grid_shape,10) # reshape as 9x9x10 tensor for easier visualisation\n",
    "        optmodel.setSLNPredictions(sln_predictions)\n",
    "\n",
    "        # Solve and compute CNN loss\n",
    "        loss = spop(predictions, c, w, z, reduction=\"mean\")\n",
    "        \n",
    "        # backward pass for CNN\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Fine-tune SLN using the differing digits from the solved visual sudoku\n",
    "        differing_digit_indices = optmodel._getDifferingDigitIndices()\n",
    "        sln_diff_digits_preds = torch.stack([sln_predictions[index] for index in differing_digit_indices])\n",
    "        diff_digits_labels = torch.stack([w.view(9,9,10)[index] for index in differing_digit_indices]).cpu()\n",
    "        sln_loss = sln_criterion(sln_diff_digits_preds, diff_digits_labels)\n",
    "        optimizer_sln.zero_grad()\n",
    "        sln_loss.backward()\n",
    "        optimizer_sln.step()\n",
    "\n",
    "        # Print progress\n",
    "        print(\"Epoch {:2},  Batch: {:2},  Loss: {:}\".format(epoch+1, i+1, loss.item()))\n",
    "        if i % 250 == 249:\n",
    "            regret, sln_loss = regretForSymoblicFeedbackWithSLNLoss(predmodel, sln, optmodel, validation_loader)\n",
    "            regret_list.append(regret)\n",
    "            print(\"Regret: \", regret, \" SLN loss: \", sln_loss.item())\n",
    "\n",
    "        # Break for early stopping if needed\n",
    "        # if i == 99:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a8c24-e95d-409f-9fe4-56dcbe90d54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training CNN and SLN, pairwise\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(5896)\n",
    "np.random.seed(5896)\n",
    "\n",
    "# init prediction model\n",
    "predmodel = PieceByPieceBoardCNN10(use_softmax=True).to(device)\n",
    "predmodel.load_state_dict(torch.load('PieceByPieceBoardCNN10_pairwise_acc_0.966_simultaneous_3.pth'))\n",
    "\n",
    "# init SLN\n",
    "sln = TinyLeNet().to(device)\n",
    "sln.load_state_dict(torch.load('TinyLeNet_SLN_0.957_simultaneous_3.pth'))\n",
    "sln_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set optimizers\n",
    "optimizer = torch.optim.Adam(predmodel.parameters(), lr=1e-7)\n",
    "optimizer_sln = torch.optim.Adam(sln.parameters(), lr=1e-4)\n",
    "\n",
    "# init optimization model\n",
    "optmodel = sudokuModelWithSymbolicFeedback(sln)\n",
    "\n",
    "# init pairwise learning to rank\n",
    "empty_dataset = pyepo.data.dataset.optDataset(optmodel, visual_train_sudokus[0:10], [transformToOneHot(solution).view(810) for solution in numerical_train_sudokus[0:10]])\n",
    "prltr = pyepo.func.pairwiseLTR(optmodel, processes=1, solve_ratio=0.5, dataset=empty_dataset)\n",
    "\n",
    "# init other\n",
    "regret_list = []\n",
    "batch_size = 1  # Has to remain 1\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validationset, batch_size=batch_size, shuffle=False)\n",
    "num_epochs = 1\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, c, w, z = data\n",
    "        x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "        \n",
    "        # forward pass CNN\n",
    "        predictions = predmodel(x)\n",
    "        \n",
    "        # SLN prediction\n",
    "        visual_sudoku = x[0]\n",
    "        grid_shape = visual_sudoku.shape[:2]\n",
    "        # reshape from 9x9x1x28x28 to 81x1x28x28 \n",
    "        pred = sln(visual_sudoku.flatten(0,1))\n",
    "        pred = pred.view(9,9,10)\n",
    "        pred = F.softmax(pred, dim=2)\n",
    "        pred = pred.cpu()  # Move the tensor to the CPU\n",
    "        # our NN return 81 probabilistic vector: an 81x10 matrix\n",
    "        sln_predictions =  pred.reshape(*grid_shape,10) # reshape as 9x9x10 tensor for easier visualisation\n",
    "        optmodel.setSLNPredictions(sln_predictions.detach())\n",
    "\n",
    "        # Solve and compute CNN loss\n",
    "        loss = prltr(predictions, c)\n",
    "        \n",
    "        # backward pass for CNN\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Fine-tune SLN using the differing digits from the solved visual sudoku\n",
    "        differing_digit_indices = optmodel._getDifferingDigitIndices()\n",
    "        sln_diff_digits_preds = torch.stack([sln_predictions[index] for index in differing_digit_indices])\n",
    "        diff_digits_labels = torch.stack([w.view(9,9,10)[index] for index in differing_digit_indices]).cpu()\n",
    "        sln_loss = sln_criterion(sln_diff_digits_preds, diff_digits_labels)\n",
    "        optimizer_sln.zero_grad()\n",
    "        sln_loss.backward()\n",
    "        optimizer_sln.step()\n",
    "\n",
    "        # Print progress\n",
    "        print(\"Epoch {:2},  Batch: {:2},  Loss: {:}\".format(epoch+1, i+1, loss.item()))\n",
    "        if i % 250 == 249:\n",
    "            regret, sln_loss = regretForSymoblicFeedbackWithSLNLoss(predmodel, sln, optmodel, validation_loader)\n",
    "            regret_list.append(regret)\n",
    "            print(\"Regret: \", regret, \" SLN loss: \", sln_loss.item())\n",
    "\n",
    "        # Break for early stopping if needed\n",
    "        # if i == 99:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ecaa0-ca81-40f1-9ba1-44fec8b0d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predmodel\n",
    "\n",
    "validationset_evaluate = CustomSudokuDatasetDFL(visual_validation_sudokus[0:200], numerical_validation_sudokus[0:200])\n",
    "validation_loader_evaluate = DataLoader(validationset_evaluate, batch_size=batch_size, shuffle=False)\n",
    "regret, sln_loss = regretForSymoblicFeedbackWithSLNLoss(predmodel, sln, optmodel, validation_loader_evaluate)\n",
    "print(\"Regret: \", regret, \" SLN loss: \", sln_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c3c2a-85b7-40fd-ba78-01dca674fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual saving\n",
    "\n",
    "torch.save(predmodel.state_dict(), 'PieceByPieceBoardCNN10_SPOP.pth')\n",
    "torch.save(sln.state_dict(), 'TinyLeNet_SLN_simultaneous.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860805f-c857-4f0b-b6f0-b984211ce5db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c7416-b3a3-4b84-8962-1971b477650f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "# Manual loading\n",
    "trained_cnn = PieceByPieceBoardCNN10()\n",
    "trained_cnn.load_state_dict(torch.load('PieceByPieceBoardCNN10.pth'))\n",
    "trained_cnn.eval()\n",
    "#trained_cnn = predmodel\n",
    "trained_sln = TinyLeNet()\n",
    "trained_sln.load_state_dict(torch.load('TinyLeNet.pth'))\n",
    "trained_sln.eval()\n",
    "\n",
    "total = 0\n",
    "normal_correct = 0\n",
    "feedback_correct = 0\n",
    "four_boards_correct = 0\n",
    "normal_correct_feedback_not = 0\n",
    "feedback_correct_normal_not = 0\n",
    "\n",
    "for i in range(0, 1):\n",
    "    numerical_sudoku = numerical_validation_sudokus[i]\n",
    "    visual_sudoku = visual_validation_sudokus[i]\n",
    "\n",
    "    normal_result = inference_simple(visual_sudoku, trained_cnn)[\"perception\"]\n",
    "    feedback_result = inference_with_feedback_double_cnn_simple(visual_sudoku, trained_cnn, trained_sln, 3)[\"perception\"]\n",
    "    four_boards = top_k_inference_simple(visual_sudoku, trained_cnn, k=3)\n",
    "\n",
    "    normal_correct_bool = False\n",
    "    feedback_correct_bool = False\n",
    "    \n",
    "    if np.array_equal(numerical_sudoku, normal_result):\n",
    "        normal_correct += 1\n",
    "        normal_correct_bool = True\n",
    "    if np.array_equal(numerical_sudoku, feedback_result):\n",
    "        feedback_correct += 1\n",
    "        feedback_correct_bool = True\n",
    "    for board in four_boards:\n",
    "        if np.array_equal(numerical_sudoku, board[\"perception\"]):\n",
    "            four_boards_correct += 1\n",
    "            break\n",
    "\n",
    "    total += 1\n",
    "    if normal_correct_bool and not feedback_correct_bool:\n",
    "        normal_correct_feedback_not += 1\n",
    "        print(\"score: \", normal_correct_feedback_not, feedback_correct_normal_not)\n",
    "    elif feedback_correct_bool and not normal_correct_bool:\n",
    "        feedback_correct_normal_not += 1\n",
    "        print(\"score: \", normal_correct_feedback_not, feedback_correct_normal_not)\n",
    "    print(total, normal_correct/total, feedback_correct/total, four_boards_correct/total)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db895d-49a0-4dba-8347-79245abfc2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# Manual loading\n",
    "trained_cnn = PieceByPieceBoardCNN10()\n",
    "trained_cnn.load_state_dict(torch.load('PieceByPieceBoardCNN10_SPOP.pth'))\n",
    "trained_cnn.eval()\n",
    "trained_sln = TinyLeNet()\n",
    "trained_sln.load_state_dict(torch.load('TinyLeNet_SLN_simultaneous.pth'))\n",
    "trained_sln.eval()\n",
    "\n",
    "total = 0\n",
    "normal_correct = 0\n",
    "feedback_correct = 0\n",
    "four_boards_correct = 0\n",
    "normal_correct_feedback_not = 0\n",
    "feedback_correct_normal_not = 0\n",
    "\n",
    "for i in range(0, 2000):\n",
    "    numerical_sudoku = numerical_test_sudokus[i]\n",
    "    visual_sudoku = visual_test_sudokus[i]\n",
    "\n",
    "    normal_result = inference_simple(visual_sudoku, trained_cnn)[\"perception\"]\n",
    "    feedback_result = inference_with_feedback_double_cnn_simple(visual_sudoku, trained_cnn, trained_sln, 3)[\"perception\"]\n",
    "    four_boards = top_k_inference_simple(visual_sudoku, trained_cnn, k=3)\n",
    "\n",
    "    normal_correct_bool = False\n",
    "    feedback_correct_bool = False\n",
    "    \n",
    "    if np.array_equal(numerical_sudoku, normal_result):\n",
    "        normal_correct += 1\n",
    "        normal_correct_bool = True\n",
    "    if np.array_equal(numerical_sudoku, feedback_result):\n",
    "        feedback_correct += 1\n",
    "        feedback_correct_bool = True\n",
    "    for board in four_boards:\n",
    "        if np.array_equal(numerical_sudoku, board[\"perception\"]):\n",
    "            four_boards_correct += 1\n",
    "            break\n",
    "\n",
    "    total += 1\n",
    "    if normal_correct_bool and not feedback_correct_bool:\n",
    "        normal_correct_feedback_not += 1\n",
    "        print(\"score: \", normal_correct_feedback_not, feedback_correct_normal_not)\n",
    "    elif feedback_correct_bool and not normal_correct_bool:\n",
    "        feedback_correct_normal_not += 1\n",
    "        print(\"score: \", normal_correct_feedback_not, feedback_correct_normal_not)\n",
    "    print(total, normal_correct/total, feedback_correct/total, four_boards_correct/total)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d7da1-2926-4f4a-bdcc-65f469dabe6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test accuracy of CNN on digits of the visual sudoku boards\n",
    "\n",
    "cnn = predmodel\n",
    "\n",
    "total = 0\n",
    "cnn_correct = 0\n",
    "for i in range(0, 2000):\n",
    "    numerical_sudoku = numerical_test_sudokus[i]\n",
    "    visual_sudoku = visual_test_sudokus[i]\n",
    "\n",
    "    ml_predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "\n",
    "    _, cnn_prediction = torch.max(torch.tensor(ml_predictions).data, -1)\n",
    "    cnn_prediction = cnn_prediction.numpy()\n",
    "    non_zero = (numerical_sudoku != 0).sum().item()\n",
    "    cnn_correct += ((cnn_prediction == numerical_sudoku.numpy())).sum().item()\n",
    "    \n",
    "    total += 81\n",
    "    print(i, cnn_correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f251c-e0b9-4ddb-bdaa-d82b02007869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test naive board accuracy\n",
    "\n",
    "cnn = predmodel\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "feedback_correct = 0\n",
    "\n",
    "for i in range(0, 2000):\n",
    "    numerical_sudoku = numerical_test_sudokus[i]\n",
    "    visual_sudoku = visual_test_sudokus[i]\n",
    "    \n",
    "    predictions = predict_proba_sudoku(cnn, visual_sudoku)\n",
    "    most_likely_label = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    if np.array_equal(numerical_sudoku, most_likely_label):\n",
    "        correct += 1\n",
    "    total +=1\n",
    "\n",
    "    print(i, correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
